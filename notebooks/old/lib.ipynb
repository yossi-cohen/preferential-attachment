{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6e118a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aef392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Math, Latex\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras;\n",
    "from tensorflow.keras.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, MaxPooling1D, Dropout, BatchNormalization, Conv1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fix random_state ?\n",
    "SEED = 17\n",
    "RANDOM_STATE = None\n",
    "RS = None\n",
    "\n",
    "def reset_random_state(random_state=None):\n",
    "    if None == random_state:\n",
    "        RANDOM_STATE = None\n",
    "        RS = None\n",
    "        return\n",
    "    \n",
    "    RANDOM_STATE = random_state\n",
    "    RS = np.random.RandomState(RANDOM_STATE)\n",
    "    os.environ['PYTHONHASHSEED']=str(RANDOM_STATE)\n",
    "    tf.random.set_seed(RANDOM_STATE)\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    random.seed(RANDOM_STATE)\n",
    "\n",
    "# reset_random_state(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64243cf",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdc510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_scale(H, C=0):\n",
    "    # log scale H rows, shift H values by (1.0+C) to avoid taking log(0)\n",
    "    axis = len(H.shape) - 1 # 1 if 2D 0 if 1D\n",
    "    return np.apply_along_axis(func1d=lambda a: np.log10(a + 1.0 + C), axis=axis, arr=H)\n",
    "\n",
    "def generate_data(N, M, nextConfig, sample, \n",
    "                  nbins=-1, # calculate from samples\n",
    "                  density=False, \n",
    "                  apply_log_scale=False):\n",
    "    \"\"\"\n",
    "    This function generates synthetic data for a learning process by means of simulation.\n",
    "    \n",
    "    The data consists of ITEMS. Each ITEM is a pair: Histogram and a Configuration.\n",
    "    \n",
    "        - Histogram: Consists of sample points (observations).\n",
    "                     The sample is drawn from a distribution using the corresponding Configuration.\n",
    "    \n",
    "        - Configuration: Distribution parameter setting for a specific sample, e.g., (alpha, loc)\n",
    "    \n",
    "    Returns:\n",
    "        - A matrix [N, num_bins] of histograms and \n",
    "        - An array of corresponding configurations (one for each histogram row).\n",
    "          num_bins == max(<observations from all samples>)\n",
    "\n",
    "    Arguments:\n",
    "        - N: number of ITEMS to generate (rows)\n",
    "        - M: number of observations that make a histogram (columns)\n",
    "        - nextConfig: A generator that returns a new configuration (alpha, loc, ...), in each call.\n",
    "          Example:\n",
    "              def yulesimon_nextConfig():\n",
    "                  alpha = random(2.0, 3.0)\n",
    "                  loc = random(0.0, 10.0)\n",
    "                  return alpha, loc\n",
    "        - sample(config, size): returns a sample with <size> data points (observations) from a distribution.\n",
    "          Example:\n",
    "              def yulesimon_sample(config, size):\n",
    "                  return yulesimon(alpha=config[0], loc=config[1], size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate samples:\n",
    "    samples = np.zeros((N, M), dtype=int)\n",
    "    configurations = np.zeros((N,), dtype=object)\n",
    "    \n",
    "    # repeat N times: draw a sample points (M observations) from distribution\n",
    "    for i in range(N):\n",
    "        \n",
    "        # start item\n",
    "        if callable(nextConfig):\n",
    "            # nextConfig is a generator\n",
    "            config = nextConfig()\n",
    "        else:\n",
    "            # in case we pass a config as a single number\n",
    "            config = nextConfig\n",
    "\n",
    "        # sample data points with current config and add to sample_matrix\n",
    "        samples[i, :] = sample(config, size=M)\n",
    "        \n",
    "        # append corresponding config\n",
    "        configurations[i] = config\n",
    "    \n",
    "    # create a histogram for each row\n",
    "    if nbins < 0:\n",
    "        nbins = np.max(samples)\n",
    "    \n",
    "    histogram_matrix = np.apply_along_axis(\n",
    "       lambda a: np.histogram(a, bins=nbins, range=(0,nbins), density=density)[0], 1, samples)\n",
    "\n",
    "    if apply_log_scale:\n",
    "        histogram_matrix = log_scale(histogram_matrix)\n",
    "    \n",
    "    return samples, histogram_matrix, configurations\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902af362",
   "metadata": {},
   "source": [
    "# DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf82484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-output regression involves predicting two or more numerical variables.\n",
    "def create_dnn_model(n_features, \n",
    "                     layers, \n",
    "                     n_outputs=1, \n",
    "                     activation='relu', \n",
    "                     init='he_uniform', \n",
    "                     batch_normalization=False, \n",
    "                     dropout=0, \n",
    "                     k_reg=False, \n",
    "                     k_reg_lr=0.001, \n",
    "                     a_reg=False, \n",
    "                     a_reg_lr=0.001, \n",
    "                     loss='mae', \n",
    "                     optimizer='adam'):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ============\n",
    "    # input-layer\n",
    "    # ============\n",
    "    model.add(Dense(units=layers[0]\n",
    "                      , input_dim=n_features\n",
    "                      , kernel_initializer=init\n",
    "                      , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                      , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                      , use_bias=(not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    if dropout >= 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # ==============\n",
    "    # hidden-layers\n",
    "    # ==============\n",
    "    if len(layers) > 1:\n",
    "        for units in layers[1:]:\n",
    "            model.add(Dense(units=units\n",
    "                            , kernel_initializer=init\n",
    "                            , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                            , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                            , use_bias=(not batch_normalization)\n",
    "                            ))\n",
    "\n",
    "            if batch_normalization:\n",
    "                model.add(BatchNormalization())\n",
    "\n",
    "            model.add(Activation(activation))\n",
    "\n",
    "            if dropout > 0:\n",
    "                model.add(Dropout(dropout))\n",
    "\n",
    "    # =============\n",
    "    # output-layer\n",
    "    # =============\n",
    "    model.add(Dense(n_outputs))\n",
    "\n",
    "    model.compile(loss=loss, metrics=[loss], optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa939fe7",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d187a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, batch_size=32):\n",
    "\n",
    "    # split train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                      y_train, \n",
    "                                                      test_size=0.25, \n",
    "                                                      random_state=RANDOM_STATE)\n",
    "\n",
    "    # early-stopping\n",
    "    es_patience = 50\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                        patience=es_patience, \n",
    "                        mode='min', \n",
    "                        restore_best_weights=True, \n",
    "                        verbose=0)\n",
    "    \n",
    "    # reduce learning-rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10)\n",
    "    \n",
    "    # model checkpoint\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    date_str = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')   \n",
    "    base_path           = f'models/DNN_{date_str}'\n",
    "    model_path          = f'{base_path}.h5'\n",
    "    history_path        = f'{base_path}.history'\n",
    "    \n",
    "    cp = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode='min', save_best_only=True, verbose=0)\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        epochs=200, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        callbacks=[es, reduce_lr, cp], \n",
    "                        verbose=0)\n",
    "    \n",
    "    # save history with model\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # load best weights from last checkpoint\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ab375",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f558d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, train_key='loss', val_key='val_loss'):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.plot(history[train_key])\n",
    "    plt.plot(history[val_key])\n",
    "    plt.title('learning curves')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6c8fa",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55cba58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dnn_model, X_test, y_test):\n",
    "\n",
    "    # predict\n",
    "    y_pred = dnn_model.predict(X_test).flatten()\n",
    "\n",
    "    # mse\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # sqrt_mse\n",
    "    sqrt_mse = np.sqrt(mse)\n",
    "\n",
    "    return y_pred, sqrt_mse\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b779d45533b5c6c3f8167369b5fb9305f3978c8e947a1d85123271165eb50113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
