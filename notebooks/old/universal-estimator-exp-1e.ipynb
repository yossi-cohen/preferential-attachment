{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa5ccd46",
   "metadata": {},
   "source": [
    "# universal estimator-exp-1e (compare to prev test-set)\n",
    "\n",
    "Let $f(d)$ be a one dimensional function, that returns a samples drawn from a univariate distribution (e.g., log-normal).\n",
    "\n",
    "**Research question**: Let $e = (\\vec{pred\\_params} - \\vec{test\\_params})$.  \n",
    "Does $STD(e)$ decreases when the parameter-search-space gets smaller?\n",
    "\n",
    "1. Generate a sample (256 observations) using $f$ e.g: $sample = f(d=0.92, size=256)$\n",
    "2. $estimator(f, sample)$ is a function which learns the parameter $d$ of $f$ from the sample.\n",
    "\n",
    "> - Init: search_space = [0, 1]\n",
    ">\n",
    "> - Iterate:\n",
    ">   1. Generate synthetic data-set ( train / test ) using $f(d)$ with two settings (exp-1a, exp-1b):\n",
    ">          d ~ uniform(search_space)\n",
    ">          d ~ norm(search_space)\n",
    ">   2. Fit a DNN model to the training set\n",
    ">   3. Compare the current model error on the test-set to the error on the previous test-set:\n",
    ">          test_params = [current test params]\n",
    ">          test_params_prev = [test params from previous iteration]\n",
    ">          e = (pred_params - test_params)\n",
    ">          sigma = STD(e)\n",
    ">          e_prev = (pred_params - test_params_prev)\n",
    ">          sigma_prev = STD(e_prev)\n",
    ">   3. Predict the parameter $d\\_pred$ on the input sample using the DNN model\n",
    ">          d_pred = model.predict(sample)\n",
    ">   4. Narrow the search space:\n",
    ">          pivot = d_pred\n",
    ">          scale = 3 * std(e)\n",
    ">          search_space = [ pivot - scale, pivot + scale ]\n",
    ">          if next_search_space == search_space: narrow the search_space by epsilon = 0.003\n",
    ">          (here I'm interested merely on narrowing the search_space, not on \"correct\" focusing)\n",
    ">   5. Let: *threshold* = 0.02.  \n",
    ">      Stop if abs( *sigma* - *sigma_prev* ) > *threshold*\n",
    "\n",
    "> - Return:\n",
    ">   - d_pred_array: array of d_pred predicted at each iteration (3)\n",
    ">   - search_spaces: array of search-space at each iteration (4)\n",
    ">   - (pred-set, test-set) at each iteration\n",
    "\n",
    "3. Plot a graph:\n",
    "\n",
    ">   - $x$: iteration #\n",
    ">   - $y1$: sigma_1 = STD(pred_params - test_params)\n",
    ">   - $y2$: abs(d_pred - d_true)\n",
    ">\n",
    "> Plot (shaded) intervals around sigma_1:\n",
    ">\n",
    "> - sigma_2:\n",
    ">          mu = mean(e)\n",
    ">          var_2 = 1/n * sum( ( (e - mu) ^ 2 - (sigma_1) ^ 2 ) ^ 2 )\n",
    ">          sigma_2 = sqrt(var_2)\n",
    ">\n",
    "> - 3 * sigma_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f29c961",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "%run lib.ipynb\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3a2fdb",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "# sample from lognormal\n",
    "def sample_lognormal(config, size):\n",
    "    return lognorm.rvs(s=config, size=size, random_state=RANDOM_STATE)\n",
    "\n",
    "def next_config(search_space):\n",
    "\n",
    "    \"\"\"\n",
    "    return a (uniform) random parameter within search_space\n",
    "    \"\"\"\n",
    "    low = search_space[0]\n",
    "    high = search_space[1]\n",
    "    return np.random.uniform(low, high, size=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf76c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS = 346\n",
    "\n",
    "def exp_1e(f, sample, d_true, initial_search_space):\n",
    "    \n",
    "    \"\"\"\n",
    "    Learn the parameter of f, from the sample.\n",
    "    Arguments:\n",
    "        - f: one dimensional function that gives the PMF of a univariate distribution.\n",
    "        - sample: generated using f.\n",
    "        - search_space: initial search-space\n",
    "    \"\"\"\n",
    "\n",
    "    # number of observations in sample\n",
    "    M = len(sample)\n",
    "    N = 1000\n",
    "   \n",
    "    # Generate a histogram for the input *sample*\n",
    "    nbins = 256\n",
    "    H_sample = np.histogram(sample, bins=nbins, range=(0, nbins), density=False)[0]\n",
    "    H_sample = np.reshape(H_sample, (1, -1))\n",
    "    \n",
    "    search_space = initial_search_space\n",
    "    first_iteration = True\n",
    "    H_test_prev = None\n",
    "    test_params_prev = None\n",
    "    threshold = 0.2\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        # 1. Generate synthetic data-sets (train/test) using f (within search_space)\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        print()\n",
    "        print(f'*** search_space: {search_space}')\n",
    "        print(f'generating data (M={M}, N={N})', end=', ')\n",
    "\n",
    "        raw_train, H_train, train_params = generate_data(N=N, \n",
    "                                       M=M, \n",
    "                                       sample=f, \n",
    "                                       nextConfig=lambda: next_config(search_space),\n",
    "                                       nbins=nbins)\n",
    "        raw_test, H_test, test_params = generate_data(N=N, \n",
    "                                       M=M, \n",
    "                                       sample=f, \n",
    "                                       nextConfig=lambda: next_config(search_space),\n",
    "                                       nbins=nbins)\n",
    "        if first_iteration:\n",
    "            # first iteration only\n",
    "            H_test_prev = H_test\n",
    "            test_params_prev = test_params\n",
    "            first_iteration = False\n",
    "\n",
    "        # 2. Fit a DNN model to train-set and predict on test-set\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "\n",
    "        # train\n",
    "        print(f'training ...', end=' ')\n",
    "        dnn_model, history = dnn_fit(X_train=H_train, y_train=train_params)\n",
    "\n",
    "        # 3. Compare the current model error on the test-set to the error on the previous test-set\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        \n",
    "        # predict test\n",
    "        pred_params = dnn_model.predict(H_test).flatten()\n",
    "\n",
    "        # error\n",
    "        e = pred_params - test_params\n",
    "        std = np.std(e)\n",
    "\n",
    "        # predict test_prev\n",
    "        pred_params_prev = dnn_model.predict(H_test_prev).flatten()\n",
    "\n",
    "        # error\n",
    "        e_prev = pred_params_prev - test_params_prev\n",
    "        std_prev = np.std(e_prev)\n",
    "        \n",
    "        std_diff = np.abs(std - std_prev)\n",
    "        print(f'*** std: {std:.4f}, std_prev: {std_prev:.4f}, std_diff: {std_diff:.4f}')\n",
    "        if std_diff > threshold:\n",
    "            break\n",
    "\n",
    "        # 4. Predict the parameter (d_pred) on the input sample\n",
    "        # ---------------------------------------------------------------------------\n",
    "\n",
    "        d_pred = dnn_model.predict(H_sample).flatten()[0]\n",
    "            \n",
    "        # 5. Narrow the search-space\n",
    "        # ---------------------------------------------------------------------------\n",
    "        search_pivot = d_pred\n",
    "        search_STD = np.std(pred_params - test_params)\n",
    "\n",
    "#         std_factor = 1\n",
    "#         std_factor = 2\n",
    "        std_factor = 3\n",
    "\n",
    "        search_scale = std_factor * search_STD\n",
    "        print(f'search_pivot: {search_pivot:.4f}, search_scale ({std_factor}*STD) = {search_scale:.4f}')\n",
    "        \n",
    "        next_search_space = np.array([ \n",
    "            max(search_space[0], search_pivot - search_scale), \n",
    "            min(search_space[1], search_pivot + search_scale)])\n",
    "        \n",
    "        # if no change in search_space, narrow by epsilon\n",
    "        if np.array_equal(search_space, next_search_space):\n",
    "            epsilon = min(0.003, 0.1 * (search_space[1] - search_space[0]))\n",
    "            print(f'no change in search_space. narrowing by epsilon: {epsilon}')\n",
    "            next_search_space = np.array([ search_space[0] + epsilon, search_space[1] - epsilon ])\n",
    "        \n",
    "        search_space = next_search_space\n",
    "        \n",
    "    print(f'd_pred: {d_pred:.4f}, abs(d_pred - d_true): {abs(d_pred - d_true):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b402dd",
   "metadata": {},
   "source": [
    "## Fit (lognormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e14faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "param true value: 0.92\n",
      "\n",
      "*** search_space: [0. 1.]\n",
      "generating data (M=256, N=1000), training ... *** std: 0.0625, std_prev: 0.0625, std_diff: 0.0000\n",
      "search_pivot: 0.8684, search_scale (3*STD) = 0.1876\n",
      "\n",
      "*** search_space: [0.6808 1.    ]\n",
      "generating data (M=256, N=1000), training ... *** std: 0.0572, std_prev: 0.2143, std_diff: 0.1571\n",
      "search_pivot: 0.9052, search_scale (3*STD) = 0.1716\n",
      "\n",
      "*** search_space: [0.7336 1.    ]\n",
      "generating data (M=256, N=1000), training ... *** std: 0.0556, std_prev: 0.2197, std_diff: 0.1640\n",
      "search_pivot: 0.8630, search_scale (3*STD) = 0.1669\n",
      "no change in search_space. narrowing by epsilon: 0.003\n",
      "\n",
      "*** search_space: [0.7366 0.997 ]\n",
      "generating data (M=256, N=1000), training ... *** std: 0.0563, std_prev: 0.2340, std_diff: 0.1777\n",
      "search_pivot: 0.8528, search_scale (3*STD) = 0.1689\n",
      "no change in search_space. narrowing by epsilon: 0.003\n",
      "\n",
      "*** search_space: [0.7396 0.994 ]\n",
      "generating data (M=256, N=1000), training ... *** std: 0.0565, std_prev: 0.2585, std_diff: 0.2020\n",
      "d_pred: 0.8528, abs(d_pred - d_true): 0.0672\n"
     ]
    }
   ],
   "source": [
    "# d_true  = 0.92\n",
    "for d_true in [0.92]:\n",
    "    print()\n",
    "    print(f'param true value: {d_true}')\n",
    "    f = sample_lognormal\n",
    "    sample = f(config=d_true, size=256)\n",
    "    initial_search_space=np.array([0.0, 1.0])\n",
    "    exp_1e(f=f, sample=sample, d_true=d_true, initial_search_space=initial_search_space)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
