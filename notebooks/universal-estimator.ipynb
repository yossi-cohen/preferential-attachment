{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fbfe3f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Universal estimator\n",
    "\n",
    "Let $f(x|d_1,...,d_n)$ be a function, that for any fixed values of the parameters $d_i$, reduces to a PDF of x; $f$ thus is a family of functions, e.g., *log normal*.\n",
    "\n",
    "$estimator(f, sample)$ is a function which learns the parameters $d_i$ of $f$ from a single input *sample* ($m$ observations drawn using $f$).\n",
    "\n",
    "## Method\n",
    "> 1. Generate synthetic *samples*.\n",
    "> 2. Learn a DNN model from the synthetic data.\n",
    "> 3. Predict the parameters $d_i$ on the input *sample*.\n",
    "\n",
    "## Parameter range adjustment\n",
    "The *estimator* is allowed to assume that the range of the parameter values is $(-\\frac{1}{2}, \\frac{1}{2})$.\n",
    "\n",
    "For that we use *Range Adjustment*:\n",
    "> Input:\n",
    "> 1. A funation $f(x|d_i)$ as defined above\n",
    "> 2. Bounds on the parameters $d_i$\n",
    "\n",
    "> Output:\n",
    "> - A function $g(x|t_i)$, which is the same as $f$, except that $t_i$ are in the range $(-\\frac{1}{2}, \\frac{1}{2})$, \n",
    "> and such that $t_i$ are ‚Äútypically‚Äù around 0.\n",
    "\n",
    "> Let: $h(d)$ be a mapping function from $range(d)$ to the range $(-\\frac{1}{2}, \\frac{1}{2})$, where $h$ is continuous within $range(d)$.\n",
    ">\n",
    "> $h^{-1}(t)$ is therefore a mapping function from the range $(-\\frac{1}{2}, \\frac{1}{2})$ to $range(d)$.\n",
    ">\n",
    "> Let: $g(x|t_i) = f(x|h^{-1}(t_i))$ where $h^{-1}(t_i)$ is a inverse of the *mapping* $h(d)$ of the bounds on the parameters $d_i$.\n",
    "\n",
    "We use the *range adjustment* as follows:\n",
    "> 1. Generate synthetic *samples* using $f(x|h^{-1}(t_i))$ where $t_i$ are drawn from $(-\\frac{1}{2}, \\frac{1}{2})$.\n",
    "> 2. Learn a DNN model from the synthetic data.\n",
    "> 3. Predict the parameters $d_i$ on the input *sample*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881b07b",
   "metadata": {},
   "source": [
    "### Range adjustment using $logit(x)$\n",
    "The standard *logistic* function is defined as: $ùúé(x) = 1 / (1 + e^{-x})$ for $x ‚àà (-‚àû, ‚àû)$.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/logistic.png\" alt=\"logistic(x)\" title=\"logistic(x), x ‚àà (‚àí‚àû,‚àû)\" /></p>\n",
    "\n",
    "The *logit* is the inverse of the *logistic* function.\n",
    "\n",
    "The *logit* function is defined as: $logit(x) = ùúé^{-1}(x) = ln( x / (1-x) )$ for $x ‚àà (0,1)$.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"images/logit.png\" alt=\"logit\" title=\"logit(x), x ‚àà (0,1)\" /></p>\n",
    "\n",
    "We can use the *logistic* function and its *logit* inverse as follows:\n",
    "> Let: $h(d) = logit(d)$ for $d ‚àà (0,1)$\n",
    ">\n",
    "> Than $h^{-1}(t) = ùúé(t)$ for $t ‚àà (-‚àû, ‚àû)$\n",
    "\n",
    "> We define a function to return another function, i.e., an adjuster of a parameter from the range $(0, 1)$ \n",
    ">\n",
    "> to the original range $(low, high)$ that a function takes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d84851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# from scipy.special import expit, logit\n",
    "\n",
    "def expit(x):\n",
    "    # expit(x) = 1 / (1 + e^(-x))\n",
    "    # defined in (-INF, INF)\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def logit(p):\n",
    "    # logit(p) = inv(expit) = ln(p/(1-p))\n",
    "    # defined in (0, 1)\n",
    "    if 0 == p: return -math.inf\n",
    "    if 1 == p: return math.inf\n",
    "    return math.log(p/(1-p))\n",
    "\n",
    "def adjuster_logit(low, high):\n",
    "    # adjuster is defined in (-INF, INF)\n",
    "    # low: parameter lower bound\n",
    "    # high: parameter higher bound\n",
    "    # return: a function defined in (0,1) that maps it's parameter (x) to the original range (low,high).\n",
    "\n",
    "    LOW = 0 if -math.inf == low else expit(low)\n",
    "    HIGH = 1 if math.inf == high else expit(high)\n",
    "\n",
    "    def adjust(x):\n",
    "        # adjust is defined in (0, 1)\n",
    "        if x < 0: return -math.inf\n",
    "        if x > 1: return math.inf\n",
    "        return logit(LOW + x * (HIGH - LOW))\n",
    "    \n",
    "    return adjust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d885e1",
   "metadata": {},
   "source": [
    "### Range adjustment using $arctan(x)$\n",
    "> <img src=\"images/tan.png\" />\n",
    "> <img src=\"images/arctan.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3be9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjuster_arctan(low, high):\n",
    "    # adjuster is defined in (-INF, INF)\n",
    "    # low: parameter lower bound\n",
    "    # high: parameter higher bound\n",
    "    # return: a function defined in (0,1) that maps it's parameter (x) to the original range (low,high).\n",
    "\n",
    "    LOW = -math.pi/2 if -math.inf == low else math.atan(low)\n",
    "    HIGH = math.pi/2 if math.inf == high else math.atan(high)\n",
    "\n",
    "    def adjust(x):\n",
    "        # adjust is defined in (0, 1)\n",
    "        if x < 0: return -math.inf\n",
    "        if x > 1: return math.inf\n",
    "        return math.tan(LOW + x * (HIGH - LOW))\n",
    "    \n",
    "    return adjust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a897e-9654-432b-914d-2f4d7b541064",
   "metadata": {},
   "source": [
    "### Test adjusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c1d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjuster_logit(2,10)(0): -2.0\n",
      "adjuster_logit(2,10)(1): 10.00000000000097\n",
      "adjuster_arctan(2,10)(0): -1.9999999999999996\n",
      "adjuster_arctan(2,10)(1): 10.00000000000001\n"
     ]
    }
   ],
   "source": [
    "print(f\"adjuster_logit(2,10)(0): {adjuster_logit(low=-2, high=10)(0)}\")\n",
    "print(f\"adjuster_logit(2,10)(1): {adjuster_logit(low=-2, high=10)(1)}\")\n",
    "print(f\"adjuster_arctan(2,10)(0): {adjuster_arctan(low=-2, high=10)(0)}\")\n",
    "print(f\"adjuster_arctan(2,10)(1): {adjuster_arctan(low=-2, high=10)(1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b2a0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust = adjuster_logit(low=0, high=10.0)\n",
    "# adjust( np.random.uniform(0.0, 1.0, size=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736504a8",
   "metadata": {},
   "source": [
    "# Universal estimator\n",
    "\n",
    "-  Apply 2D  range adjustment; for now, it will be independent parameters: should adjust to $(-\\frac{1}{2}, \\frac{1}{2})^2, (n=2)$.  \n",
    "   In this case: we want the range to be an n-dimensional unit cube; this makes it easier to understand subranges: e.g., 0.1-0.2.  \n",
    "   We also introduce some \"prior\" in a very convoluted way. A priori, the range is -‚àû to +‚àû and we know nothing about where the parameter is. After range adjustment, we shrink some regions by a large factor and others by a small factor. So, we favor the center of the range. We may even beat Cramer-Rao for this reason.\n",
    "\n",
    "-  Select 5 or so longtail distributions (we may need to change the families later). Do range adjustment, and it should be sensible in the sense that it is approximately centered.\n",
    "\n",
    "- Do range adjustment (orthogonal for now, i.e, independently for each original parameter).\n",
    "\n",
    "- Draw a mesh on the unit square, say with a resolution of 0.1, this gives 81 points for each mesh. Maybe 1/11 resolution? Maybe include the last and first row and column. \n",
    "\n",
    "- For each point in the mesh, run learning experiment producing:\n",
    "   - Several (say 10-100) learning experiments. without focusing for now.\n",
    "   - Compute the average error of the 10-100 experiments. MSE rules\n",
    "   - Compute the Cramer Rao bound (numerically if the analytical solution is not possible)\n",
    "   - Divide the MSE by Cramer Rao.\n",
    "  \n",
    "- Draw the 3D manifold on the mesh.\n",
    "\n",
    "### Expected results:\n",
    "\n",
    "1. Hopefully, we are at most 2 times worse than CR. \n",
    "2. Hopefully, the CR is never singular, i.e. not zero nor infinity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa550000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def generate_data(N, M, sample, param_ranges, nbins=-1):\n",
    "    \"\"\"\n",
    "    param_ranges: [dX2] array of parameter ranges (e.g. for 2 parmeters: [[0, 10], [0, math.inf]])\n",
    "    \"\"\"\n",
    "    \n",
    "    # output (generate samples)\n",
    "    samples = np.zeros((N, M), dtype=float)\n",
    "    \n",
    "    # generate N parameter cubes in the range (-0.5, +0.5)\n",
    "    params = np.zeros((N, 2), dtype=float)\n",
    "\n",
    "    param_adjusters = [adjuster_logit(low=pr[0], high=pr[1]) for pr in param_ranges]\n",
    "    \n",
    "    n_params = param_ranges.shape[0]\n",
    "    \n",
    "    for i in range(N):\n",
    "        for param_i in range(n_params):\n",
    "            adjuster = param_adjusters[param_i]\n",
    "            # ti is in (-0.5, 0.5)\n",
    "            ti = np.random.uniform(0.0, 1.0, size=1)[0]\n",
    "            # di is in (di_low, di_high)\n",
    "            params[i, param_i] = adjuster(ti)\n",
    "    \n",
    "    # repeat N times: draw a sample from distribution (M observations)\n",
    "    for i in range(N):\n",
    "        samples[i, :] = sample(params[i], size=M)\n",
    "    \n",
    "    # create a histogram from each sample\n",
    "    if nbins < 0:\n",
    "        nbins = int(np.max(samples))+1\n",
    "    \n",
    "    histogram_matrix = np.apply_along_axis(\n",
    "       lambda a: np.histogram(a, bins=nbins, range=(0, nbins), density=False)[0], 1, samples)\n",
    "\n",
    "    return samples, params, histogram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d46743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 55 119  48  18   8   4   2   1   0   0   1   0   0   0   0]\n",
      " [  0 256   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0  58 118  52  12   7   1   3   3   1   1   0   0   0   0]\n",
      " [ 69 149  32   6   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 229  27   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [112  97  30  12   4   0   0   0   0   0   0   0   0   0   1]\n",
      " [  0   1 144  92  15   3   1   0   0   0   0   0   0   0   0]\n",
      " [  0 256   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "def lognormal_sample(params, size):\n",
    "    return lognorm.rvs(s=params[0], loc=params[1], size=size)\n",
    "\n",
    "sample = lognormal_sample\n",
    "ranges = np.array(\n",
    "    [[0.0, 1.0], \n",
    "     [0.0, math.inf]]\n",
    ")\n",
    "\n",
    "samples, params, histogram_matrix = generate_data(\n",
    "    N=8, M=256, sample=sample, param_ranges=ranges)\n",
    "\n",
    "# print(samples)\n",
    "# print(params)\n",
    "print(histogram_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdbad834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 184  41 ...   0   0   0]\n",
      " [  0   0 183 ...   0   0   0]\n",
      " [  0   0 155 ...   0   0   1]\n",
      " ...\n",
      " [  0   0 181 ...   0   0   0]\n",
      " [  0 161  58 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import yulesimon\n",
    "\n",
    "def yulesimon_sample(params, size):\n",
    "    return yulesimon.rvs(alpha=params[0], loc=params[1], size=size)\n",
    "\n",
    "sample = yulesimon_sample\n",
    "ranges = np.array(\n",
    "    [[2.0, 3.0], \n",
    "     [0.0, math.inf]]\n",
    ")\n",
    "\n",
    "samples, params, histogram_matrix = generate_data(\n",
    "    N=8, M=256, sample=sample, param_ranges=ranges)\n",
    "\n",
    "# print(samples)\n",
    "# print(params)\n",
    "print(histogram_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc8d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dnn library\n",
    "%run dnn.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
