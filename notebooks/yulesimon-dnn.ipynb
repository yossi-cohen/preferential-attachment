{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install sklearn\n",
    "# !pip install keras\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess data\n",
    "- read csv files into a single dataframe\n",
    "- normalize data\n",
    "- shuffle samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducability\n",
    "def fix_random(seed):\n",
    "    os.environ['PYTONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "fix_random(seed=0)\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X_train.shape: (6000, 1000)\nX_val.shape: (2000, 1000)\nX_test.shape: (2000, 1000)\n"
    }
   ],
   "source": [
    "def read_csvs(csv_files, verbose=True):\n",
    "    '''Read a list of csv files into a single DataFrame'''\n",
    "    df = pd.DataFrame()\n",
    "    for csv in csv_files:\n",
    "        if verbose:\n",
    "            print('loading:', csv)\n",
    "        df = df.append(pd.read_csv(csv, header=None, index_col=False))\n",
    "    return df\n",
    "\n",
    "data = 'data'\n",
    "csv_files = [os.path.join(data, f) for f in os.listdir(data)]\n",
    "df = read_csvs(csv_files, verbose=False)\n",
    "df = shuffle(df, random_state=RANDOM_STATE)\n",
    "y = df.iloc[:,0]\n",
    "X = df.iloc[:,1:]\n",
    "\n",
    "# split train/validation/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RANDOM_STATE)\n",
    "print('X_train.shape: {}'.format(X_train.shape))\n",
    "print('X_val.shape: {}'.format(X_val.shape))\n",
    "print('X_test.shape: {}'.format(X_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0  1  2  3   4   5\n0  1  1  1  2   1  10\n1  1  1  1  1  10   2\n\nh1 = histogram(df)\n[8 2 0 0 0 0 0 0 0 2]\n\nh2 = histogram(h1, density=True)\n[0.875 0.    0.25  0.    0.    0.    0.    0.    0.    0.125]\n\nh3 = log(h1)\n[2.07944154 0.69314718 0.         0.         0.         0.\n 0.         0.         0.         0.69314718]\n"
    }
   ],
   "source": [
    "df = pd.DataFrame([[1, 1, 1, 2, 1, 10], [1, 1, 1, 1, 10, 2]])\n",
    "print(df)\n",
    "\n",
    "print()\n",
    "print('h1 = histogram(df)')\n",
    "h1, _ = np.histogram(df)\n",
    "print(h1)\n",
    "\n",
    "print()\n",
    "print('h2 = histogram(h1, density=True)')\n",
    "h2, _ = np.histogram(h1, density=True)\n",
    "print(h2)\n",
    "\n",
    "print()\n",
    "print('h3 = log(h1)')\n",
    "h3 = np.log(np.maximum(h1, 1))\n",
    "print(h3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_features, \n",
    "                 layers, \n",
    "                 activation='relu', \n",
    "                 init='he_uniform', \n",
    "                 batch_normalization=False, \n",
    "                 dropout=0, \n",
    "                 optimizer='adam', \n",
    "                 optimizer_lr=0.01, \n",
    "                 k_reg=False, \n",
    "                 k_reg_lr=0.001, \n",
    "                 a_reg=False, \n",
    "                 a_reg_lr=0.001, \n",
    "                 metrics=['mse']):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ============\n",
    "    # input-layer\n",
    "    # ============\n",
    "    model.add(Dense(units=layers[0], \n",
    "                      input_dim=n_features, \n",
    "                      kernel_initializer=init, \n",
    "                      activation=activation))\n",
    "                      # kernel_regularizer=l2(k_reg_lr) if k_reg else None, \n",
    "                      # activity_regularizer=l2(a_reg_lr) if a_reg else None)\n",
    "    \n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout())\n",
    "\n",
    "    # ==============\n",
    "    # hidden-layers\n",
    "    # ==============\n",
    "    for units in layers[1:]:\n",
    "        model.add(Dense(units=units, \n",
    "                        kernel_initializer=init, \n",
    "                        kernel_regularizer=l2(k_reg_lr) if k_reg else None, \n",
    "                        activity_regularizer=l2(a_reg_lr) if a_reg else None))\n",
    "\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout())\n",
    "\n",
    "    # =============\n",
    "    # output-layer\n",
    "    # =============\n",
    "    model.add(Dense(units=1, kernel_initializer=init))\n",
    "                  # kernel_regularizer=l2(k_reg_lr) if k_reg else None, \n",
    "                  # activity_regularizer=l2(a_reg_lr) if a_reg else None)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\nWall time: 13.1 µs\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "MODEL_PREFIX = 'yulesimon'\n",
    "\n",
    "def run(X_train, y_train):\n",
    "    model = create_model(X_train.shape[1], \n",
    "                         layers=[64, 64, 64], \n",
    "                         activation='relu', \n",
    "                         init='he_uniform', \n",
    "                         batch_normalization=False, \n",
    "                         dropout=0.15, \n",
    "                         optimizer='adam', \n",
    "                         optimizer_lr=0.0001, \n",
    "                         k_reg=False, \n",
    "                         k_reg_lr=1e-5, \n",
    "                         a_reg=False, \n",
    "                         a_reg_lr=1e-6, \n",
    "                         metrics=['mse'])\n",
    "    \n",
    "    # split train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
    "    \n",
    "    # early-stopping\n",
    "    es_patience = 100\n",
    "    es_ = EarlyStopping(monitor='val_loss', \n",
    "                        patience=es_patience, \n",
    "                        mode='min', \n",
    "                        restore_best_weights=True, \n",
    "                        verbose=1)\n",
    "    \n",
    "    # model checkpoint\n",
    "    date_str = datetimet.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "    model_path = 'models/{}_{}.h5'.format(MODEL_PREFIX, date_str)\n",
    "    print('model path:', model_path)\n",
    "    \n",
    "    cp = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "    \n",
    "    # reduce learning-rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=min(30, int(es_patience/2)))\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        ephochs=10000, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, callbacks=[es, reduce_lr, cp], \n",
    "                        verbose=1)\n",
    "    \n",
    "    history_unique_name = 'models/{}_{}.history'.format(MODEL_PREFIX, date_str)\n",
    "    with open(history_unique_name, 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "    \n",
    "    # load best weights from last checkpoint\n",
    "    model = keras.models.load_model(model_path)\n",
    "        \n",
    "    return model, history\n",
    "\n",
    "if False:\n",
    "    run(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}