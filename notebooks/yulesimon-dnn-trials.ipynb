{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import yulesimon\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "# fix random seed for reproducability\n",
    "def fix_random(seed):\n",
    "    os.environ['PYTONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "fix_random(seed=0)\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_data(num_alphas, samples_per_alpha, rvs_size):\n",
    "    '''\n",
    "    params:\n",
    "        num_alphas: number of alphas generated between MIN_ALPHA and MAX_ALPHA inclusive\n",
    "        samples_per_alpha: number of samples (rows) for each alpha \n",
    "        rvs_size: number of RV samples (columns) per row\n",
    "    '''\n",
    "\n",
    "    MIN_ALPHA = 2.01\n",
    "    MAX_ALPHA = 3.00\n",
    "\n",
    "    # fix loc at zero\n",
    "    loc = 0\n",
    "\n",
    "    X = np.empty((num_alphas * samples_per_alpha, 1 + rvs_size), float)\n",
    "\n",
    "    row = 0\n",
    "    for alpha in np.linspace(MIN_ALPHA, MAX_ALPHA, num=num_alphas):\n",
    "        \n",
    "        #lilo: alpha = round(alpha, 2)\n",
    "        \n",
    "        # generate samples (rows) for current alpha\n",
    "        for i in range(samples_per_alpha):\n",
    "            X[row, 0] = alpha\n",
    "            X[row, 1:] = yulesimon.rvs(alpha, loc=loc, size=rvs_size, random_state=RANDOM_STATE)\n",
    "            row += 1\n",
    "\n",
    "    # suffle rows\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    # separate X from y\n",
    "    y = X[:, 0]\n",
    "    X = X[:, 1:].astype(int)\n",
    "\n",
    "    # convert rows of X to a histogram H\n",
    "    nbins = np.max(X)\n",
    "    H = np.apply_along_axis(lambda a: np.histogram(a, bins=nbins, density=False)[0], 1, X)\n",
    "\n",
    "    # log scale H rows\n",
    "    logH = np.apply_along_axis(lambda a: np.log10(a+1), 1, H)\n",
    "    return logH, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_features, \n",
    "                 layers, \n",
    "                 activation='relu', \n",
    "                 init='he_uniform', \n",
    "                 batch_normalization=False, \n",
    "                 dropout=0, \n",
    "                 optimizer='adam', \n",
    "                 k_reg=False, \n",
    "                 k_reg_lr=0.001, \n",
    "                 a_reg=False, \n",
    "                 a_reg_lr=0.001):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ============\n",
    "    # input-layer\n",
    "    # ============\n",
    "    model.add(Dense(units=layers[0]\n",
    "                      , input_dim=n_features\n",
    "                      , kernel_initializer=init\n",
    "                      , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                      , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                      , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # ==============\n",
    "    # hidden-layers\n",
    "    # ==============\n",
    "    for units in layers[1:]:\n",
    "        model.add(Dense(units=units\n",
    "                        , kernel_initializer=init\n",
    "                        , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                        , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                        , use_bias= (not batch_normalization)\n",
    "                        ))\n",
    "\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # =============\n",
    "    # output-layer\n",
    "    # =============\n",
    "    model.add(Dense(units=1\n",
    "                    , kernel_initializer=init\n",
    "                    , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                    , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                    , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    # if dropout > 0:\n",
    "    #     model.add(Dropout(dropout))\n",
    "\n",
    "    model.compile(loss='mse', metrics=['mse'], optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MODEL_PREFIX = 'yulesimon'\n",
    "\n",
    "def train(X_train, y_train, X_val, y_val):\n",
    "    model = create_model(X_train.shape[1], \n",
    "                         layers=[256, 256], \n",
    "                         activation='relu', \n",
    "                         init=keras.initializers.HeUniform(seed=RANDOM_STATE), \n",
    "                         batch_normalization=True, \n",
    "                         dropout=0.0, \n",
    "                         optimizer=Adam(lr=1e-2), \n",
    "                         #optimizer=SGD(lr=1e-3, momentum=0.9, decay=0.01), \n",
    "                         k_reg=True, \n",
    "                         k_reg_lr=1e-5, \n",
    "                         a_reg=True, \n",
    "                         a_reg_lr=1e-5)\n",
    "    \n",
    "    # early-stopping\n",
    "    es_patience = 50\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                        patience=es_patience, \n",
    "                        mode='min', \n",
    "                        restore_best_weights=True, \n",
    "                        verbose=0)\n",
    "    \n",
    "    # model checkpoint\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    date_str = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "    base_path       = 'models/{}_{}'.format(MODEL_PREFIX, date_str)\n",
    "    model_path      = '{}.h5'.format(base_path)\n",
    "    history_path    = '{}.history'.format(base_path)\n",
    "    \n",
    "    cp = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode='min', save_best_only=True, verbose=0)\n",
    "    \n",
    "    # reduce learning-rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        epochs=3000, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, \n",
    "                        callbacks=[es, reduce_lr, cp], \n",
    "                        verbose=0)\n",
    "    \n",
    "    # save history with model\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # load best weights from last checkpoint\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trail(num_alphas, samples_per_alpha, rvs_size):\n",
    "    print('trail( num_alphas={}, samples_per_alpha={}, rvs_size={} )'.format(num_alphas, samples_per_alpha, rvs_size))\n",
    "    \n",
    "    X, y = generate_data(num_alphas=num_alphas, samples_per_alpha=samples_per_alpha, rvs_size=rvs_size)\n",
    "\n",
    "    # split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "    # split train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "    # training\n",
    "    model, history = train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # predict\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "    # evaluate\n",
    "    squared_error = np.square(y_test - y_pred)\n",
    "    mse = (squared_error).mean(axis=0)\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "trail( num_alphas=100, samples_per_alpha=100, rvs_size=1000 )\nsqrt(mse): 0.010401112748193435\ntrail( num_alphas=100, samples_per_alpha=1000, rvs_size=1000 )\nsqrt(mse): 0.017349139086295727\n"
    }
   ],
   "source": [
    "sqrt_mse = trail(num_alphas=100, samples_per_alpha=100, rvs_size=1000)\n",
    "print('sqrt(mse):', sqrt_mse)\n",
    "\n",
    "sqrt_mse = trail(num_alphas=100, samples_per_alpha=1000, rvs_size=1000)\n",
    "print('sqrt(mse):', sqrt_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "trail( num_alphas=256, samples_per_alpha=100, rvs_size=1000 )\nsqrt(mse): 0.012142827534484518\n"
    }
   ],
   "source": [
    "sqrt_mse = trail(num_alphas=256, samples_per_alpha=100, rvs_size=1000)\n",
    "print('sqrt(mse):', sqrt_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "trail( num_alphas=256, samples_per_alpha=100, rvs_size=100 )\nsqrt(mse): 0.02033064700188158\n"
    }
   ],
   "source": [
    "sqrt_mse = trail(num_alphas=256, samples_per_alpha=100, rvs_size=100)\n",
    "print('sqrt(mse):', sqrt_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}