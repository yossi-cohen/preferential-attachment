{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import library\n",
    "%run lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data (yulesimon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import yulesimon\n",
    "\n",
    "def sample_yulesimon(alpha, size):\n",
    "    return yulesimon.rvs(alpha, loc=0, size=size)\n",
    "\n",
    "def next_alpha(min_alpha=2.0, max_alpha=3.0):\n",
    "    alpha = np.random.uniform(low=min_alpha, high=max_alpha, size=1)[0]\n",
    "    return alpha\n",
    "\n",
    "# generate data (yulesimon)\n",
    "def generate_data_yulesimon(N, M, density=False):\n",
    "    H, alphas = generate_data(N=N, M=M, nextConfig=next_alpha, sample=sample_yulesimon)\n",
    "\n",
    "    # split train/test\n",
    "    # (use train_test_split so the shape of the train/test data will be the same)\n",
    "    H_train, H_test, y_train, y_test = train_test_split(H, alphas, test_size=0.25)\n",
    "    \n",
    "    return H_train, y_train, H_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Experiment\n",
    "Run an experiment to show that using the log function improves something, either accuracy or learning rate, or both.\n",
    "Use fixed setting, e.g, YS, samples = 256 or 512.\n",
    "In this experiment use the LOG(1+X) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(H):\n",
    "    # normalize values (sum to 1)\n",
    "    return H / H.sum(axis=1, keepdims=1)\n",
    "\n",
    "def log_sacle(H):\n",
    "    # log scale H rows\n",
    "    # (shift H values by one so as not to take log of zero)\n",
    "    return np.apply_along_axis(lambda a: np.log10(a), 1, H + 1)\n",
    "\n",
    "def Experiment_1(M_values, N=10000):\n",
    "\n",
    "    trials_out = []\n",
    "\n",
    "    d = {\n",
    "        'M': [], \n",
    "        'm': [],\n",
    "        'log-scale': [],\n",
    "        'sqrt-mse': [],\n",
    "        'train-time (sec.)': []\n",
    "    }\n",
    "    \n",
    "    for M in M_values:\n",
    "        \n",
    "        print()\n",
    "        print(f'generating dataset (M: {M}) ... ', end='')\n",
    "        X_train, y_train, X_test, y_test = generate_data_yulesimon(N=N, M=M)\n",
    "        print(f'train input.shape: {X_train.shape}')\n",
    "        m = X_train.shape[1]\n",
    "\n",
    "        \n",
    "        #for density in [True, False]:\n",
    "        for density in [False]:\n",
    "\n",
    "            if density:\n",
    "                X_train = normalize(X_train)\n",
    "                X_test = normalize(X_test)\n",
    "\n",
    "            for log_scale in [True, False]:\n",
    "\n",
    "                if log_scale:\n",
    "                    X_train = log_sacle(X_train)\n",
    "                    X_test = log_sacle(X_test)\n",
    "\n",
    "                print(f'training - log-scale: {log_scale}', end=' ... ')\n",
    "\n",
    "                start_time = time.time()\n",
    "                dnn_model, history, y_pred, sqrt_mse = dnn_trial(X_train, y_train, X_test, y_test)\n",
    "                train_time = round(time.time() - start_time)\n",
    "                \n",
    "                # debug print\n",
    "                loss = np.min(history['loss'])\n",
    "                val_loss = np.min(history['val_loss'])\n",
    "                print(f'sqrt-mse: {sqrt_mse:.3f}', end=', ')\n",
    "                print(f'loss: {loss:.3f}', end=', ')\n",
    "                print(f'val_loss: {val_loss:.3f}', end=', ')\n",
    "                print(f'train-time: {round(train_time)} sec.')\n",
    "\n",
    "                trials_out.append({\n",
    "                    'model': dnn_model,\n",
    "                    'history': history,\n",
    "                    'y_test': y_test,\n",
    "                    'y_pred': y_pred,\n",
    "                    'sqrt-mse': sqrt_mse\n",
    "                })\n",
    "                \n",
    "                d['M'].append(M)\n",
    "                d['m'].append(m)\n",
    "                d['log-scale'].append(log_scale)\n",
    "                d['sqrt-mse'].append(round(sqrt_mse, 3))\n",
    "                d['train-time (sec.)'].append(f'{train_time}')\n",
    "\n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return trials_out, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_out, df = Experiment_1(M_values = [256, 512, 1024, 2048, 4096, 8192, 16384])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log shift experiment\n",
    "Run an experiment to see which C is better if using the function LOG(1+C)\n",
    "Try, e.g., C = 0.2, 0.5, 1, 2,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possion experiment\n",
    "Learn Poisson in three different settings:\n",
    "\n",
    "Use exact values.  \n",
    "Use Log.  \n",
    "Use P values, defined from the poisson thingy.  \n",
    "P(k) = Log(MEASURED(k) * k!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias experiment\n",
    "Same experiment as usual, but make sure the prediction is unbiased. Average difference should be close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
