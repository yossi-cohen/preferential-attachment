{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import library\n",
    "%run lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data (yulesimon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import yulesimon\n",
    "\n",
    "def sample_yulesimon(alpha, size):\n",
    "    return yulesimon.rvs(alpha, loc=0, size=size)\n",
    "\n",
    "def next_alpha(min_alpha=2.0, max_alpha=3.0):\n",
    "    alpha = np.random.uniform(low=min_alpha, high=max_alpha, size=1)[0]\n",
    "    return alpha\n",
    "\n",
    "# generate data (yulesimon)\n",
    "def generate_data_yulesimon(N, M, density=False):\n",
    "    H, alphas = generate_data(N=N, M=M, nextConfig=next_alpha, sample=sample_yulesimon)\n",
    "\n",
    "    # split train/test\n",
    "    # (use train_test_split so the shape of the train/test data will be the same)\n",
    "    H_train, H_test, y_train, y_test = train_test_split(H, alphas, test_size=0.25)\n",
    "    \n",
    "    return H_train, y_train, H_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials: train on H vs. on logH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generating dataset (M: 256)... training input.shape: (7500, 2462)\n",
      "training - log-scale: True... "
     ]
    }
   ],
   "source": [
    "def normalize(H):\n",
    "    # normalize values (sum to 1)\n",
    "    return H / H.sum(axis=1, keepdims=1)\n",
    "\n",
    "def log_sacle(H):\n",
    "    # log scale H rows\n",
    "    # (shift H values by one so as not to take log of zero)\n",
    "    return np.apply_along_axis(lambda a: np.log10(a), 1, H + 1)\n",
    "\n",
    "def trial1(M_values):\n",
    "\n",
    "    trials_out = []\n",
    "\n",
    "    d = {\n",
    "        'M': [], \n",
    "        'm': [],\n",
    "        'log-scale': [],\n",
    "        #'density': [], #lilo:density\n",
    "        'sqrt-mse': [],\n",
    "        'train-time (sec.)': []\n",
    "    }\n",
    "    \n",
    "    N = 10000\n",
    "    \n",
    "    for M in M_values:\n",
    "        \n",
    "        print()\n",
    "        print(f'generating dataset (M: {M}) ... ', end='')\n",
    "        X_train, y_train, X_test, y_test = generate_data_yulesimon(N=N, M=M)\n",
    "        print(f'training input.shape: {X_train.shape}')\n",
    "        m = X_train.shape[1]\n",
    "\n",
    "        \n",
    "        #lilo:density\n",
    "        #for density in [True, False]:\n",
    "        for density in [False]:\n",
    "\n",
    "            if density:\n",
    "                X_train = normalize(X_train)\n",
    "                X_test = normalize(X_test)\n",
    "\n",
    "            for log_scale in [True, False]:\n",
    "\n",
    "                if log_scale:\n",
    "                    X_train = log_sacle(X_train)\n",
    "                    X_test = log_sacle(X_test)\n",
    "\n",
    "                print(f'training - log-scale: {log_scale}', end=' ... ')\n",
    "\n",
    "                start_time = time.time()\n",
    "                dnn_model, history, y_pred, sqrt_mse = dnn_trial(X_train, y_train, X_test, y_test)\n",
    "                train_time = round(time.time() - start_time)\n",
    "                \n",
    "                # debug print\n",
    "                loss = np.min(history['loss'])\n",
    "                val_loss = np.min(history['val_loss'])\n",
    "                print(f'sqrt-mse: {sqrt_mse:.3f}', end=', ')\n",
    "                print(f'loss: {loss:.3f}', end=', ')\n",
    "                print(f'val_loss: {val_loss:.3f}', end=', ')\n",
    "                print(f'train-time: {round(train_time)} sec.')\n",
    "\n",
    "                # plot learning curves\n",
    "                #plot_learning_curves(history)\n",
    "\n",
    "                trials_out.append({\n",
    "                    'model': dnn_model,\n",
    "                    'history': history,\n",
    "                    'y_test': y_test,\n",
    "                    'y_pred': y_pred,\n",
    "                    'sqrt-mse': sqrt_mse\n",
    "                })\n",
    "                \n",
    "                d['M'].append(M)\n",
    "                d['m'].append(m)\n",
    "                d['log-scale'].append(log_scale)\n",
    "                #d['density'].append(density) #lilo:density\n",
    "                d['sqrt-mse'].append(round(sqrt_mse, 3))\n",
    "                d['train-time (sec.)'].append(f'{train_time}')\n",
    "\n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return trials_out, df\n",
    "\n",
    "trials_out, df = trial1(M_values = [256, 512, 1024, 2048, 4096, 8192])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runing for M in [256, 512, 1024, 2048, 4096]  \n",
    "For each M run with density=True/False  \n",
    "Measure execution time  \n",
    "\n",
    "We can see that:  \n",
    "1. MSE decreases for larger M\n",
    "2. Training time on logM is silightly better then training on M\n",
    "\n",
    "| M    | train | sqrt_mse | time | density | m    |\n",
    "| ---  | ---   | ---      | ---  | ---     | ---  |\n",
    "|  256 | H     | 0.209    | 1m   | True    | 2243 |\n",
    "|  256 | logH  | 0.198    | 1m   | True    | 2243 |\n",
    "|  256 | H     | 0.209    | 1m   | False   | 1577 |\n",
    "|  256 | logH  | 0.205    | 30s  | False   | 1577 |\n",
    "|  512 | H     | 0.168    | 60s  | True    | 3530 |\n",
    "|  512 | logH  | 0.162    | 2m   | True    | 3530 |\n",
    "|  512 | H     | 0.163    | 50s  | False   |  880 |\n",
    "|  512 | logH  | 0.174    | 20s  | False   |  880 |\n",
    "| 1024 | H     | 0.114    | 2-4m | True    | 2700 |\n",
    "| 1024 | logH  | 0.114    | 1-3m | True    | 2700 |\n",
    "| 1024 | H     | 0.124    | 38s  | False   | 1500 |\n",
    "| 1024 | logH  | 0.141    | 29s  | False   | 1500 |\n",
    "| 2048 | H     | 0.101    | 53s  | True    | 2306 |\n",
    "| 2048 | logH  | 0.089    | 1m   | True    | 2306 |\n",
    "| 2048 | H     | 0.091    | 2m   | False   | 2674 |\n",
    "| 2048 | logH  | 0.102    | 45s  | False   | 2674 |\n",
    "| 4096 | H     | 0.071    | 2.5m | True    | 7467 |\n",
    "| 4096 | logH  | 0.066    | 3m   | True    | 7467 |\n",
    "| 4096 | H     | 0.068    | 3.5m | False   | 5367 |\n",
    "| 4096 | logH  | 0.083    | 2m   | False   | 5367 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
