{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35762d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "%run lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023fb420",
   "metadata": {},
   "source": [
    "## Experiment 3.1: Compute prediction error\n",
    "So far, we computed the error (MSE / MAE) of λ comparing the learned one with the actual one.  \n",
    "In this experiment, we test the error in prediction. So,\n",
    "\n",
    "### Part I\n",
    "- M=256 sample points, generating histogram H of size K; H[k] = number of samples in bin k, i.e., count how many sample points. Therefore K is computed, but will be small for typical values of λ.\n",
    "- λ = 0.5 ... 1.5\n",
    "- N = 10K\n",
    "\n",
    "Let V = (H[k] - M * Poisson(λ,k))\n",
    "\n",
    "Then V is parameterized by\n",
    "\n",
    "k: the current bin\n",
    "\n",
    "i = 1,...,N: sample number\n",
    "\n",
    "All computation must include therefore K*N values of V.\n",
    "\n",
    "**Objective**: compute MAE(V), MSE(V), MEAN_BIAS(V), with respect to all these values.\n",
    "\n",
    "### Part II\n",
    "Repeat with λ computed by the expectation of the sample, i.e., $\\sum_{k=0}^{K}\\; \\frac{k*H[k]}{M}$\n",
    "\n",
    "### Part III\n",
    "- Combine Part I and Part II, to find out which is better; ours should be. We need a single table/graph/plot.\n",
    "\n",
    "\n",
    "- Show that the expectation of the sample, gives a biased estimate, and that ours does not.\n",
    "\n",
    "  To do so: Same data as stage I, but NO LEARNING. Use N=10,000, compute λ by its expectation; \n",
    "  \n",
    "  compare with the λ you selected at random, and show that there is bias between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578bd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poisson_data(N, M, min_lambda=0.5, max_lambda=1.5):\n",
    "    \n",
    "    # define λ generator for 0.5 <= λ <= 1.5\n",
    "    lambda_gen = lambda: next_lambda(min_lambda=min_lambda, max_lambda=max_lambda)\n",
    "\n",
    "    raw, H, lambdas = generate_data(N=N, \n",
    "                                    M=M, \n",
    "                                    nextConfig=lambda_gen,\n",
    "                                    sample=sample_poisson, \n",
    "                                    density=False, \n",
    "                                    dense_histogram=True, \n",
    "                                    apply_log_scale=False)\n",
    "    return H, lambdas\n",
    "\n",
    "def compute_V(H, M, lambdas):\n",
    "    i,j = np.ix_(np.arange(H.shape[0]), np.arange(H.shape[1]))\n",
    "    V = H - M * poisson.pmf(k=j, mu=lambdas[i])\n",
    "    return V\n",
    "\n",
    "def experiment_3_1_part_I(H_train, H_test, lambdas_train, lambdas_test):\n",
    "    \n",
    "    # fit model\n",
    "    print(f'fitting dnn model ... ', end='')\n",
    "    start_time = time.time()\n",
    "    dnn_model, history = dnn_fit(X_train=H_train, y_train=lambdas_train)\n",
    "    train_time = round(time.time() - start_time)\n",
    "    print(f'duration: {round(train_time)} sec.')\n",
    "\n",
    "    # predict lambdas\n",
    "    print(f'predicting lambdas ... ', end='')\n",
    "    lambdas_pred, sqrt_mse = dnn_predict(dnn_model, H_test, lambdas_test)\n",
    "    print(f'sqrt_mse(λ): {sqrt_mse:.6f}')\n",
    "    \n",
    "    # compute V: V[i,j] = H[i,j] - M * Poisson(λ[i], j)\n",
    "    V = compute_V(H=H_test, M=M, lambdas=lambdas_pred)\n",
    "    \n",
    "    # MAE(V)\n",
    "    MAE = np.mean(np.abs(V))\n",
    "    print(f'MAE: {MAE:.6f}')\n",
    "    \n",
    "    # SQRT_MSE(V)\n",
    "    SQRT_MSE = np.sqrt(np.mean(np.square(V)))\n",
    "    print(f'SQRT_MSE: {SQRT_MSE:.6f}')\n",
    "    \n",
    "    # MEAN_BIAS(V)\n",
    "    MEAN_BIAS = np.mean(V)\n",
    "    print(f'MEAN_BIAS: {MEAN_BIAS:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c21b354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating data (M=256, N=10000) ... histogram shape: (7500, 11)\n",
      "\n",
      "experiment_3_1_part_I (prediced λ)\n",
      "fitting dnn model ... duration: 23 sec.\n",
      "predicting lambdas ... sqrt_mse(λ): 0.060201\n",
      "MAE: 1.782004\n",
      "SQRT_MSE: 3.399430\n",
      "MEAN_BIAS: 0.000002\n"
     ]
    }
   ],
   "source": [
    "# for reporoducable results\n",
    "reset_random_state(17)\n",
    "\n",
    "# generate data (histogram and lambdas)\n",
    "N=10000\n",
    "M=256\n",
    "min_lambda=0.5\n",
    "max_lambda=1.5\n",
    "print(f'generating data (M={M}, N={N}) ... ', end='')\n",
    "H, lambdas = generate_poisson_data(N, M, min_lambda=min_lambda, max_lambda=max_lambda)\n",
    "H_train, H_test, lambdas_train, lambdas_test = train_test_split(H, lambdas, test_size=0.25, \n",
    "                                                                random_state=RANDOM_STATE)\n",
    "print(f'histogram shape: {H_train.shape}')\n",
    "\n",
    "print()\n",
    "print(f'experiment_3_1_part_I (prediced λ)')\n",
    "experiment_3_1_part_I(H_train, H_test, lambdas_train, lambdas_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0d824",
   "metadata": {},
   "source": [
    "### Part II\n",
    "Repeat with λ computed by the expectation of the sample, i.e., $\\sum_{k=0}^{K}\\; \\frac{k*H[k]}{M}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f35323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_3_1_part_II (expected λ)\n",
      "MAE: 1.754229\n",
      "SQRT_MSE: 3.370083\n",
      "MEAN_BIAS: 0.000002\n"
     ]
    }
   ],
   "source": [
    "def calc_lambdas_expected(H, M):\n",
    "    \"\"\" return lambda expected at each row \"\"\"\n",
    "    k = np.ix_(np.arange(H_test.shape[1]))\n",
    "    return np.sum(k*H/M, axis=1)\n",
    "\n",
    "def experiment_3_1_part_II(H, M):\n",
    "    \n",
    "    lambdas_expected = calc_lambdas_expected(H, M)\n",
    "    \n",
    "    # compute V: V[i,j] = H[i,j] - M * Poisson(λ[i], j)\n",
    "    V = compute_V(H=H, M=M, lambdas=lambdas_expected)\n",
    "    \n",
    "    # MAE(V)\n",
    "    MAE = np.mean(np.abs(V))\n",
    "    print(f'MAE: {MAE:.6f}')\n",
    "    \n",
    "    # SQRT_MSE(V)\n",
    "    SQRT_MSE = np.sqrt(np.mean(np.square(V)))\n",
    "    print(f'SQRT_MSE: {SQRT_MSE:.6f}')\n",
    "    \n",
    "    # MEAN_BIAS(V)\n",
    "    MEAN_BIAS = np.mean(V)\n",
    "    print(f'MEAN_BIAS: {MEAN_BIAS:.6f}')\n",
    "\n",
    "print(f'experiment_3_1_part_II (expected λ)')\n",
    "experiment_3_1_part_II(H=H_test, M=M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a453ae",
   "metadata": {},
   "source": [
    "## Experiment 3.2: Can we help learning of Poisson, by teaching the underlying mathematics.\n",
    "\n",
    "Learning the computed λ, by using expectation is **extremely easy** for a neural network. \n",
    "Based on #7, we hope that the DNN does better than this. So force it to do better, we give it the computed λ as input, and only ask it to learn the difference between the *real* λ (as used to synthesize the data) and the *estimator* λ. \n",
    "\n",
    "Our question is: _can we help the neural network to learn the data if we give it mathematical hints on it?_\n",
    "\n",
    "To do so, define ζ to be the estimator λ, as computed by the expectation; \n",
    "\n",
    "> $ζ = Σ_{i=0}^{K} \\frac{k * H[k]}{M}$\n",
    "\n",
    "Define a new histogram Z, computed from H, using ζ .  The new histogram will show the difference between the expected value and the real value,  i.e, run the following loop for k=0, ... , K\n",
    "\n",
    "> $Z[k] = H[k] - M * Poisson(ζ,k)$\n",
    "\n",
    "Now apply DNN. The new data includes all the data the previous DNN achieved, but in a slightly more convenient way. Hopefully, the results are better.\n",
    "\n",
    "Design and implement and experiment to give a conclusive answer to this question. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d491e65",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
