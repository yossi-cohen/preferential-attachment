{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import yulesimon\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, MaxPooling1D, Dropout, BatchNormalization, Conv1D, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.metrics import mean_squared_error"
   ]
  },
  {
   "source": [
    "# fix seed for reproducability"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed):\n",
    "    os.environ['PYTONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "source": [
    "# Generate data\n",
    "### &nbsp;&nbsp; *num_alphas*: number of alphas generated between (min_alpha) and (max_alpha) inclusive\n",
    "### &nbsp;&nbsp; *samples_per_alpha*: number of samples (rows) for each alpha \n",
    "### &nbsp;&nbsp; *N* : number of random variates (number of samples drawn from yulesimon distribution)\n",
    "### &nbsp;&nbsp; *M* : maximun value of random variates (length of input vectors == number of features)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_data(N, \n",
    "                  num_alphas=100, \n",
    "                  samples_per_alpha=100, \n",
    "                  min_alpha=2.01, \n",
    "                  max_alpha=3.00, \n",
    "                  loc=0, \n",
    "                  random_alpha=True, \n",
    "                  max_M_only=False, \n",
    "                  hstack_zeros=False, \n",
    "                  random_state=0):\n",
    "    '''\n",
    "    params:\n",
    "        N: number of RV samples (columns) per row\n",
    "        num_alphas: number of alphas generated between (min_alpha) and (max_alpha) inclusive\n",
    "        samples_per_alpha: number of samples (rows) for each alpha \n",
    "    '''\n",
    "\n",
    "    if random_alpha:\n",
    "        alphas = np.random.uniform(low=min_alpha, high=max_alpha, size=num_alphas)\n",
    "    else:\n",
    "        alphas = np.linspace(min_alpha, max_alpha, num=num_alphas)\n",
    "    \n",
    "    if max_M_only:\n",
    "\n",
    "        # only return max_M\n",
    "        max_M = 0\n",
    "        for alpha in alphas:\n",
    "            for i in range(samples_per_alpha):\n",
    "                X = yulesimon.rvs(alpha, loc=loc, size=N, random_state=random_state)\n",
    "                max_M = max(max_M, np.max(X))\n",
    "        return max_M\n",
    "\n",
    "    row = 0\n",
    "    \n",
    "    X = np.empty((num_alphas * samples_per_alpha, N+1), float)\n",
    "\n",
    "    for alpha in alphas:\n",
    "        \n",
    "        # generate samples (rows) for current alpha\n",
    "        for i in range(samples_per_alpha):\n",
    "            X[row, 0] = alpha\n",
    "            X[row, 1:] = yulesimon.rvs(alpha, loc=loc, size=N, random_state=random_state)\n",
    "            row += 1\n",
    "\n",
    "    # suffle rows\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    # separate X from y\n",
    "    y = X[:, 0]\n",
    "    X = X[:, 1:].astype(int)\n",
    "\n",
    "    # create a histogram (H) from (X) rows\n",
    "    nbins = np.max(X)\n",
    "    H = np.apply_along_axis(lambda a: np.histogram(a, bins=nbins, density=False)[0], 1, X)\n",
    "\n",
    "    # log scale (H) rows\n",
    "    logH = np.apply_along_axis(lambda a: np.log10(a+1), 1, H)\n",
    "\n",
    "    # if False:\n",
    "    #     nbins2 = 150\n",
    "    #     logH2 = np.zeros((logH.shape[0], nbins2))\n",
    "    #     logH2[:,:logH.shape[1]] = logH\n",
    "    #     return logH2, y, nbins2 # (nbins == M)\n",
    "\n",
    "    if hstack_zeros:\n",
    "        NUM_ZERO_COLUMNS_TO_HSTACK = 10\n",
    "        zeros = np.zeros_like(logH, shape=(logH.shape[0], NUM_ZERO_COLUMNS_TO_HSTACK))\n",
    "        return np.hstack((logH, zeros)), y, nbins + NUM_ZERO_COLUMNS_TO_HSTACK # (nbins == M)\n",
    "\n",
    "    return logH, y, nbins # (nbins == M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_every_n_elements(a, n):\n",
    "    mod = a.shape[1] % n\n",
    "    if mod > 0:\n",
    "        a = np.hstack((a, np.zeros_like(a, shape=[a.shape[0], n - mod])))\n",
    "    else:\n",
    "        a = a.copy()\n",
    "    b = np.reshape(a, (a.shape[0], int(a.shape[1]/n), n))\n",
    "    c = np.max(b, axis=2)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_hstack_win_max(X, window_sizes=None):\n",
    "    if None == window_sizes:\n",
    "        # generate window sizes: 2, 4, 8, ..., half_len = int(X.shape[1]/2)\n",
    "        window_sizes=[]\n",
    "        half_len = int(X.shape[1]/2)\n",
    "        for i in range(half_len):\n",
    "            size = int(math.pow(2, i+1))\n",
    "            if size > half_len:\n",
    "                break\n",
    "            window_sizes.append(size)\n",
    "    \n",
    "    # start with input copy and hstack windows\n",
    "    OUT = X.copy()\n",
    "    for window_size in window_sizes:\n",
    "        tmp = max_every_n_elements(X, window_size)\n",
    "        OUT = np.hstack((OUT, tmp))\n",
    "    \n",
    "    return OUT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_hstack_win_sum(X, window_sizes=None):\n",
    "    if None == window_sizes:\n",
    "        # generate window sizes: 2, 4, 8, ..., half_len = int(X.shape[1]/2)\n",
    "        window_sizes=[]\n",
    "        half_len = int(X.shape[1]/2)\n",
    "        for i in range(half_len):\n",
    "            size = int(math.pow(2,i+1))\n",
    "            if size > half_len:\n",
    "                break\n",
    "            window_sizes.append(size)\n",
    "    \n",
    "    # start with input copy and hstack windows\n",
    "    OUT = X.copy()\n",
    "    for window_size in window_sizes:\n",
    "        mod = X.shape[1] % window_size\n",
    "        if 0 == mod:\n",
    "            tmp = np.add.reduceat(X, np.arange(0, X.shape[1], window_size), axis=1)\n",
    "        else:\n",
    "            tmp = np.add.reduceat(X[:, 0:-mod], np.arange(0, X.shape[1]-mod, window_size), axis=1)\n",
    "        OUT = np.hstack((OUT, tmp))\n",
    "    \n",
    "    return OUT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(n_features, filters=32):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(n_features, 1)))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "# Create CNN Model (multi-layer)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model_multi_layer(n_features, filters=32):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # conv layer 1\n",
    "    input_size = n_features\n",
    "    filters = 32\n",
    "    padding = 0\n",
    "    kernel_size = 7\n",
    "    strides = 1\n",
    "\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, activation=\"relu\", input_shape=(input_size, 1)))\n",
    "    \n",
    "    # conv layer 2\n",
    "    input_size = ( input_size + 2 * padding - kernel_size ) / strides  + 1\n",
    "    filters = 32\n",
    "    padding = 0\n",
    "    kernel_size = 5\n",
    "    strides = 1\n",
    "\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, activation=\"relu\", input_shape=(input_size, 1)))\n",
    "\n",
    "    # pooling layer 1\n",
    "    pool_size = 2\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    # conv layer 3\n",
    "    input_size = ( int(input_size/pool_size) + 2 * padding - kernel_size ) / strides  + 1\n",
    "    filters = 32\n",
    "    padding = 0\n",
    "    kernel_size = 3\n",
    "    strides = 1\n",
    "\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, activation=\"relu\", input_shape=(input_size, 1)))\n",
    "\n",
    "    # pooling layer 2\n",
    "    pool_size = 2\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    # flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # dense layer 1\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "# Create DNN Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn_model(n_features, \n",
    "                 layers, \n",
    "                 activation='relu', \n",
    "                 init='he_uniform', \n",
    "                 batch_normalization=False, \n",
    "                 dropout=0, \n",
    "                 optimizer='adam', \n",
    "                 k_reg=False, \n",
    "                 k_reg_lr=0.001, \n",
    "                 a_reg=False, \n",
    "                 a_reg_lr=0.001):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ============\n",
    "    # input-layer\n",
    "    # ============\n",
    "    model.add(Dense(units=layers[0]\n",
    "                      , input_dim=n_features\n",
    "                      , kernel_initializer=init\n",
    "                      , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                      , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                      , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # ==============\n",
    "    # hidden-layers\n",
    "    # ==============\n",
    "    for units in layers[1:]:\n",
    "        model.add(Dense(units=units\n",
    "                        , kernel_initializer=init\n",
    "                        , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                        , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                        , use_bias= (not batch_normalization)\n",
    "                        ))\n",
    "\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # =============\n",
    "    # output-layer\n",
    "    # =============\n",
    "    model.add(Dense(units=1\n",
    "                    , kernel_initializer=init\n",
    "                    , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                    , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                    , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mse', metrics=['mse'], optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, filters=32, batch_size=32, random_state=0):\n",
    "\n",
    "    # split train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "        test_size=0.25, random_state=random_state)\n",
    "\n",
    "    # early-stopping\n",
    "    es_patience = 50\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                        patience=es_patience, \n",
    "                        mode='min', \n",
    "                        restore_best_weights=True, \n",
    "                        verbose=0)\n",
    "    \n",
    "    # model checkpoint\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    date_str = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "    base_path       = 'models/yulesimon_{}'.format(date_str)\n",
    "    model_path      = '{}.h5'.format(base_path)\n",
    "    history_path    = '{}.history'.format(base_path)\n",
    "    \n",
    "    cp = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode='min', save_best_only=True, verbose=0)\n",
    "    \n",
    "    # reduce learning-rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        epochs=200, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        callbacks=[es, reduce_lr, cp], \n",
    "                        verbose=0)\n",
    "    \n",
    "    # save history with model\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # load best weights from last checkpoint\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, train_key='loss', val_key='val_loss'):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.plot(history[train_key])\n",
    "    plt.plot(history[val_key])\n",
    "    plt.title('learning curves')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()\n",
    "    print('loss:', np.min(history['loss']))\n",
    "    print('val_loss:', np.min(history['val_loss']))    "
   ]
  },
  {
   "source": [
    "# Trials"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(nn='CNN', N_range=[5,6], random_states=[0, 17], hstack_zeros=False):\n",
    "    print('Processing...')\n",
    "    print()\n",
    "\n",
    "    a_N = np.array([])\n",
    "    a_sqrt_mse = np.array([])\n",
    "    a_avg_abs_errors = np.array([])\n",
    "    a_std_abs_errors = np.array([])\n",
    "\n",
    "    # change N [32..2048]\n",
    "    for i in N_range:\n",
    "\n",
    "        N = 2**i\n",
    "\n",
    "        a_N = np.append(a_N, N)\n",
    "\n",
    "        # data generation (fix random state)\n",
    "        DATA_RANDOM_STATE = 0\n",
    "        fix_random(seed=DATA_RANDOM_STATE)\n",
    "        X, y, M = generate_data(N=N, \n",
    "                                num_alphas=100, \n",
    "                                samples_per_alpha=100, \n",
    "                                random_alpha=True, \n",
    "                                max_M_only=False, \n",
    "                                hstack_zeros=hstack_zeros, \n",
    "                                random_state=DATA_RANDOM_STATE)\n",
    "\n",
    "        # reshape X for Conv1D\n",
    "        if nn.startswith('CNN'):\n",
    "            X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "        elif 'DNN_WIN' == nn:\n",
    "            X = data_hstack_win_max(X, window_sizes=None)\n",
    "\n",
    "        # split train/test\n",
    "        print('input.shape:', X.shape)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=DATA_RANDOM_STATE)\n",
    "\n",
    "        # create model\n",
    "        if nn.startswith('CNNM'): # CNN-multi-layers\n",
    "            model = create_cnn_model_multi_layer(X.shape[1], filters=32)\n",
    "        elif nn.startswith('CNN'):\n",
    "            model = create_cnn_model(X.shape[1], filters=32)\n",
    "        elif nn.startswith('DNN'):\n",
    "            model = create_dnn_model(X_train.shape[1], \n",
    "                            layers=[256,256], \n",
    "                            activation='relu', \n",
    "                            init=keras.initializers.HeUniform(seed=DATA_RANDOM_STATE), \n",
    "                            batch_normalization=True, \n",
    "                            dropout=0.0, \n",
    "                            optimizer=Adam(lr=1e-2), \n",
    "                            k_reg=True, \n",
    "                            k_reg_lr=1e-5, \n",
    "                            a_reg=True, \n",
    "                            a_reg_lr=1e-5)\n",
    "        else:\n",
    "            raise RuntimeError(\"nn: '{}' not supported\".format(nn))\n",
    "\n",
    "        # average sqrt_mse among different random states (fixed N)\n",
    "        avg_sqrt_mse = 0\n",
    "\n",
    "        # array of absolute errors\n",
    "        avg_abs_errors = np.zeros_like(y_test)\n",
    "\n",
    "        # change random state\n",
    "        for rs in random_states:\n",
    "\n",
    "            # fix random\n",
    "            fix_random(seed=rs)\n",
    "        \n",
    "            # training\n",
    "            model, history = train(model, X_train, y_train, batch_size=32, random_state=rs)\n",
    "\n",
    "            # predict\n",
    "            y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "            # mse\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "            # sqrt_mse\n",
    "            sqrt_mse = np.sqrt(mse)\n",
    "\n",
    "            # avg_sqrt_mse (accumulator)\n",
    "            avg_sqrt_mse += sqrt_mse\n",
    "            \n",
    "            #print('N = {}, M = {}, random_state = {}    =>    sqrt_mse = {:.6f}'.format(N, M, rs, sqrt_mse))\n",
    "\n",
    "            # absolute errors\n",
    "            avg_abs_errors += np.abs(y_test - y_pred)\n",
    "\n",
    "        # avg errors across random_states\n",
    "        avg_abs_errors = avg_abs_errors / len(random_states)\n",
    "        \n",
    "        # avg errors across samples (t_test)\n",
    "        a_avg_abs_errors = np.append(a_avg_abs_errors, np.average(avg_abs_errors))\n",
    "\n",
    "        # std across samples\n",
    "        a_std_abs_errors = np.append(a_std_abs_errors, np.std(avg_abs_errors))\n",
    "\n",
    "        # avg_sqrt_mse\n",
    "        avg_sqrt_mse = avg_sqrt_mse / len(random_states)\n",
    "        a_sqrt_mse = np.append(a_sqrt_mse, avg_sqrt_mse)\n",
    "        print('N: {} \\t avg_sqrt_mse = {:.6f}'.format(N, avg_sqrt_mse))\n",
    "        print()\n",
    "\n",
    "    # return a_N, a_sqrt_mse, a_avg_abs_errors, a_std_abs_errors \n",
    "    return {\n",
    "        'nn': nn, \n",
    "        'a_N':a_N, \n",
    "        'a_sqrt_mse':a_sqrt_mse, \n",
    "        'a_avg_abs_errors':a_avg_abs_errors, \n",
    "        'a_std_abs_errors':a_std_abs_errors\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}