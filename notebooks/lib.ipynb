{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook is a library\n",
    "### It contains the followin functions:\n",
    "1. **generate_data**: generates data for a learning process by means of simulation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import os\r\n",
    "import time\r\n",
    "import pickle\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from IPython.display import Math, Latex\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from tensorflow import keras;\r\n",
    "from tensorflow.keras.metrics import mean_squared_error\r\n",
    "from tensorflow.keras.regularizers import l2\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\r\n",
    "from tensorflow.keras import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Activation, MaxPooling1D, Dropout, BatchNormalization, Conv1D, Flatten\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def generate_data(N, M, nextConfig, sample, density=False):\r\n",
    "    \"\"\"\r\n",
    "    This function generates data for a learning process by means of simulation.\r\n",
    "    \r\n",
    "    The data consists of ITEMS. Each ITEM is a pair: histogram and a configuration \r\n",
    "    (for now, the configuration is scalar / a floating point value).\r\n",
    "    \r\n",
    "    A histogram consists of sample points. \r\n",
    "    Each sample points is drawn from a distribution, with some specific configuration.\r\n",
    "    \r\n",
    "    Parameter setting for distribution = Configuration, eg.., alpha, gamma, alpha+LOC\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        - a matrix of histograms and \r\n",
    "          an array of corresponding configs (one config for each histogram row).\r\n",
    "\r\n",
    "    Arguments:\r\n",
    "        - N: number of ITEMS to generate (rows)\r\n",
    "        - M: number of sample points that make a histogram (columns)\r\n",
    "        - nextConfig: A generator that gives a new alpha/gamma/delta, in each call.\r\n",
    "            Example: nextConfig()\r\n",
    "                lambda = random(1,10)\r\n",
    "                return lambda\r\n",
    "        - sample(config, size): returns sample data points from a distribution (size = num samples).\r\n",
    "            Example sample(alpha, size=256) \r\n",
    "                ys = yulesimon(alpha, size)\r\n",
    "                return ys\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    items_matrix = np.zeros((N, M), dtype=int)\r\n",
    "    config_array = np.zeros((N,), dtype=float)\r\n",
    "    \r\n",
    "    # generate items_matrix:\r\n",
    "    #   repeat N times:\r\n",
    "    #     draw M sample points from distribution\r\n",
    "    for i in range(N):\r\n",
    "        \r\n",
    "        # start item\r\n",
    "        if callable(nextConfig):\r\n",
    "            config = nextConfig()\r\n",
    "        else:\r\n",
    "            config = nextConfig # in case we pass a numeric config\r\n",
    "\r\n",
    "        # sample data points with current config and add to sample_matrix\r\n",
    "        items_matrix[i, :] = sample(config, size=M)\r\n",
    "        \r\n",
    "        # append corresponding config\r\n",
    "        config_array[i] = config\r\n",
    "    \r\n",
    "    # create histograms from rows of items_matrix\r\n",
    "    nbins = np.max(items_matrix)\r\n",
    "        \r\n",
    "    histogram_matrix = np.apply_along_axis(\r\n",
    "        lambda a: np.histogram(a, bins=nbins, range=(1,nbins), density=density)[0], 1, items_matrix)\r\n",
    "\r\n",
    "    return items_matrix, histogram_matrix, config_array\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Log scale & normalize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def log_scale(H, C=0.0):\r\n",
    "    # log scale H rows\r\n",
    "    # (shift H values by one so as not to take log of zero)\r\n",
    "    return np.apply_along_axis(lambda a: np.log10(a + 1.0 + C), 1, H)\r\n",
    "\r\n",
    "def normalize(H):\r\n",
    "    # normalize values (sum to 1)\r\n",
    "    return H / H.sum(axis=1, keepdims=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DNN Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_dnn_model(n_features, \r\n",
    "                 layers, \r\n",
    "                 activation='relu', \r\n",
    "                 init='he_uniform', \r\n",
    "                 batch_normalization=False, \r\n",
    "                 dropout=0, \r\n",
    "                 optimizer='adam', \r\n",
    "                 k_reg=False, \r\n",
    "                 k_reg_lr=0.001, \r\n",
    "                 a_reg=False, \r\n",
    "                 a_reg_lr=0.001):\r\n",
    "\r\n",
    "    model = Sequential()\r\n",
    "    \r\n",
    "    # ============\r\n",
    "    # input-layer\r\n",
    "    # ============\r\n",
    "    model.add(Dense(units=layers[0]\r\n",
    "                      , input_dim=n_features\r\n",
    "                      , kernel_initializer=init\r\n",
    "                      , kernel_regularizer=l2(k_reg_lr) if k_reg else None\r\n",
    "                      , activity_regularizer=l2(a_reg_lr) if a_reg else None\r\n",
    "                      , use_bias= (not batch_normalization)\r\n",
    "                    ))\r\n",
    "    \r\n",
    "    \r\n",
    "    if batch_normalization:\r\n",
    "        model.add(BatchNormalization())\r\n",
    "    \r\n",
    "    model.add(Activation(activation))\r\n",
    "\r\n",
    "    if dropout > 0:\r\n",
    "        model.add(Dropout(dropout))\r\n",
    "\r\n",
    "    # ==============\r\n",
    "    # hidden-layers\r\n",
    "    # ==============\r\n",
    "    for units in layers[1:]:\r\n",
    "        model.add(Dense(units=units\r\n",
    "                        , kernel_initializer=init\r\n",
    "                        , kernel_regularizer=l2(k_reg_lr) if k_reg else None\r\n",
    "                        , activity_regularizer=l2(a_reg_lr) if a_reg else None\r\n",
    "                        , use_bias= (not batch_normalization)\r\n",
    "                        ))\r\n",
    "\r\n",
    "    if batch_normalization:\r\n",
    "        model.add(BatchNormalization())\r\n",
    "\r\n",
    "    model.add(Activation(activation))\r\n",
    "    \r\n",
    "    if dropout > 0:\r\n",
    "        model.add(Dropout(dropout))\r\n",
    "\r\n",
    "    # =============\r\n",
    "    # output-layer\r\n",
    "    # =============\r\n",
    "    model.add(Dense(units=1\r\n",
    "                    , kernel_initializer=init\r\n",
    "                    , kernel_regularizer=l2(k_reg_lr) if k_reg else None\r\n",
    "                    , activity_regularizer=l2(a_reg_lr) if a_reg else None\r\n",
    "                    , use_bias= (not batch_normalization)\r\n",
    "                    ))\r\n",
    "    \r\n",
    "    if batch_normalization:\r\n",
    "        model.add(BatchNormalization())\r\n",
    "\r\n",
    "    model.add(Activation('linear'))\r\n",
    "\r\n",
    "    model.compile(loss='mse', metrics=['mse'], optimizer=optimizer)\r\n",
    "\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def create_cnn_model(n_features, filters=32):\r\n",
    "    \r\n",
    "    model = Sequential()\r\n",
    "\r\n",
    "    model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(n_features, 1)))\r\n",
    "    \r\n",
    "    model.add(MaxPooling1D(pool_size=2))\r\n",
    "\r\n",
    "    model.add(Flatten())\r\n",
    "\r\n",
    "    model.add(Dense(64, activation=\"relu\"))\r\n",
    "\r\n",
    "    model.add(Dense(1))\r\n",
    "    \r\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\r\n",
    "\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_model(model, X_train, y_train, batch_size=32):\r\n",
    "\r\n",
    "    # split train/val\r\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\r\n",
    "\r\n",
    "    # early-stopping\r\n",
    "    es_patience = 50\r\n",
    "    es = EarlyStopping(monitor='val_loss', \r\n",
    "                        patience=es_patience, \r\n",
    "                        mode='min', \r\n",
    "                        restore_best_weights=True, \r\n",
    "                        verbose=0)\r\n",
    "    \r\n",
    "    # reduce learning-rate on plateau\r\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10)\r\n",
    "    \r\n",
    "    # model checkpoint\r\n",
    "    if not os.path.exists('models'):\r\n",
    "        os.makedirs('models')\r\n",
    "    date_str = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')   \r\n",
    "    base_path           = f'models/DNN_{date_str}'\r\n",
    "    model_path          = f'{base_path}.h5'\r\n",
    "    history_path        = f'{base_path}.history'\r\n",
    "    cp = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode='min', save_best_only=True, verbose=0)\r\n",
    "\r\n",
    "    # fit model\r\n",
    "    history = model.fit(X_train, \r\n",
    "                        y_train, \r\n",
    "                        validation_data=(X_val, y_val), \r\n",
    "                        epochs=200, \r\n",
    "                        batch_size=batch_size, \r\n",
    "                        shuffle=False, \r\n",
    "                        callbacks=[es, reduce_lr, cp], \r\n",
    "                        verbose=0)\r\n",
    "    \r\n",
    "    # save history with model\r\n",
    "    with open(history_path, 'wb') as f:\r\n",
    "        pickle.dump(history.history, f)\r\n",
    "    \r\n",
    "    # load best weights from last checkpoint\r\n",
    "    model = keras.models.load_model(model_path)\r\n",
    "    return model, history.history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning Curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_learning_curves(history, train_key='loss', val_key='val_loss'):\r\n",
    "    plt.figure(figsize=(2,2))\r\n",
    "    plt.plot(history[train_key])\r\n",
    "    plt.plot(history[val_key])\r\n",
    "    plt.title('learning curves')\r\n",
    "    plt.ylabel('loss')\r\n",
    "    plt.xlabel('epoch')\r\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\r\n",
    "    # plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create and train DNN model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dnn_trial(X_train, y_train, X_test, y_test):\r\n",
    "    dnn_model = create_dnn_model(X_train.shape[1], \r\n",
    "                            layers=[256,256], \r\n",
    "                            activation='relu', \r\n",
    "                            init=keras.initializers.HeUniform(), \r\n",
    "                            batch_normalization=True, \r\n",
    "                            dropout=0.0, \r\n",
    "                            optimizer=Adam(learning_rate=1e-2), \r\n",
    "                            k_reg=True,\r\n",
    "                            k_reg_lr=1e-5, \r\n",
    "                            a_reg=True,\r\n",
    "                            a_reg_lr=1e-5)\r\n",
    "\r\n",
    "    # train\r\n",
    "    dnn_model, history = train_model(dnn_model, X_train, y_train)\r\n",
    "    \r\n",
    "    # predict\r\n",
    "    y_pred = dnn_model.predict(X_test).flatten()\r\n",
    "\r\n",
    "    # mse\r\n",
    "    mse = mean_squared_error(y_test, y_pred)\r\n",
    "\r\n",
    "    # sqrt_mse\r\n",
    "    sqrt_mse = np.sqrt(mse)\r\n",
    "\r\n",
    "    return dnn_model, history, y_pred, sqrt_mse"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}