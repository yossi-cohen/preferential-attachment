{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is a library\n",
    "### It contains the followin functions:\n",
    "1. **generate_data**: generates data for a learning process by means of simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, MaxPooling1D, Dropout, BatchNormalization, Conv1D, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, M, nextConfig, sample):\n",
    "    \"\"\"\n",
    "    This function generates data for a learning process by means of simulation.\n",
    "    \n",
    "    The data consists of ITEMS. Each ITEM is a pair: histogram and a configuration \n",
    "    (for now, the configuration is scalar / a floating point value).\n",
    "    \n",
    "    A histogram consists of sample points. \n",
    "    Each sample points is drawn from a distribution, with some specific configuration.\n",
    "    \n",
    "    Parameter setting for distribution = Configuration, eg.., alpha, gamma, alpha+LOC\n",
    "    \n",
    "    Returns:\n",
    "        - a matrix of histograms and \n",
    "          an array of corresponding configs (one config for each histogram row).\n",
    "\n",
    "    Arguments:\n",
    "        - N: number of ITEMS to generate (rows)\n",
    "        - M: number of sample points that make a histogram (columns)\n",
    "        - nextConfig: A generator that gives a new alpha/gamma/delta, in each call.\n",
    "            Example: nextConfig()\n",
    "                lambda = random(1,10)\n",
    "                return lambda\n",
    "        - sample(config, size): returns sample data points from a distribution (size = num samples).\n",
    "            Example sample(alpha, size=256) \n",
    "                ys = yulesimon(alpha, size)\n",
    "                return ys\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    items_matrix = np.zeros((N, M), dtype=int)\n",
    "    config_array = np.zeros((N,), dtype=float)\n",
    "    \n",
    "    # generate items_matrix:\n",
    "    #   repeat N times:\n",
    "    #     draw M sample points from distribution\n",
    "    for i in range(N):\n",
    "        \n",
    "        # start item\n",
    "        config = nextConfig()\n",
    "\n",
    "        # sample data points with current config and add to sample_matrix\n",
    "        items_matrix[i, :] = sample(config, size=M)\n",
    "        \n",
    "        # append corresponding config\n",
    "        config_array[i] = config\n",
    "    \n",
    "    # create histograms from rows of items_matrix\n",
    "    nbins = np.max(items_matrix)\n",
    "\n",
    "#     min_item = items_matrix.min()\n",
    "#     max_item = items_matrix.max()\n",
    "#     nbins = np.arange(min_item, max_item)\n",
    "#     arange = (min_item, max_item) \n",
    "        \n",
    "    histogram_matrix = np.apply_along_axis(\n",
    "        lambda a: np.histogram(a, bins=nbins, range=(1,nbins))[0], 1, items_matrix)\n",
    "\n",
    "    return histogram_matrix, config_array\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn_model(n_features, \n",
    "                 layers, \n",
    "                 activation='relu', \n",
    "                 init='he_uniform', \n",
    "                 batch_normalization=False, \n",
    "                 dropout=0, \n",
    "                 optimizer='adam', \n",
    "                 k_reg=False, \n",
    "                 k_reg_lr=0.001, \n",
    "                 a_reg=False, \n",
    "                 a_reg_lr=0.001):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ============\n",
    "    # input-layer\n",
    "    # ============\n",
    "    model.add(Dense(units=layers[0]\n",
    "                      , input_dim=n_features\n",
    "                      , kernel_initializer=init\n",
    "                      , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                      , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                      , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # ==============\n",
    "    # hidden-layers\n",
    "    # ==============\n",
    "    for units in layers[1:]:\n",
    "        model.add(Dense(units=units\n",
    "                        , kernel_initializer=init\n",
    "                        , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                        , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                        , use_bias= (not batch_normalization)\n",
    "                        ))\n",
    "\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # =============\n",
    "    # output-layer\n",
    "    # =============\n",
    "    model.add(Dense(units=1\n",
    "                    , kernel_initializer=init\n",
    "                    , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                    , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                    , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mse', metrics=['mse'], optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(n_features, filters=32):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(n_features, 1)))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, batch_size=32):\n",
    "\n",
    "    # split train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\n",
    "\n",
    "    # early-stopping\n",
    "    es_patience = 50\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                        patience=es_patience, \n",
    "                        mode='min', \n",
    "                        restore_best_weights=True, \n",
    "                        verbose=0)\n",
    "    \n",
    "    # reduce learning-rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10)\n",
    "    \n",
    "    # model checkpoint\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    date_str = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')   \n",
    "    base_path           = 'models/DNN_{}'.format(date_str)\n",
    "    model_path          = '{}.h5'.format(base_path)\n",
    "    history_path        = '{}.history'.format(base_path)\n",
    "    cp = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode='min', save_best_only=True, verbose=0)\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        epochs=200, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        callbacks=[es, reduce_lr, cp], \n",
    "                        verbose=0)\n",
    "    \n",
    "    # save history with model\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # load best weights from last checkpoint\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, train_key='loss', val_key='val_loss'):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.plot(history[train_key])\n",
    "    plt.plot(history[val_key])\n",
    "    plt.title('learning curves')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_trial(X_train, y_train, X_test, y_test):\n",
    "    dnn_model = create_dnn_model(X_train.shape[1], \n",
    "                            layers=[256,256], \n",
    "                            activation='relu', \n",
    "                            init=keras.initializers.HeUniform(), \n",
    "                            batch_normalization=True, \n",
    "                            dropout=0.0, \n",
    "                            optimizer=Adam(learning_rate=1e-2), \n",
    "                            k_reg=True,\n",
    "                            k_reg_lr=1e-5, \n",
    "                            a_reg=True,\n",
    "                            a_reg_lr=1e-5)\n",
    "\n",
    "    # train\n",
    "    dnn_model, history = train_model(dnn_model, X_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_pred = dnn_model.predict(X_test).flatten()\n",
    "\n",
    "    # mse\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # sqrt_mse\n",
    "    sqrt_mse = np.sqrt(mse)\n",
    "\n",
    "    return dnn_model, history, y_pred, sqrt_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot histogram and log scale of a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(H):\n",
    "    \"\"\"\n",
    "    plot data:\n",
    "        - histogram\n",
    "        - log scale\n",
    "        - cumsum\n",
    "    \"\"\"\n",
    "    # plot random row\n",
    "    ROW_INDEX_TO_PLOT = random.randint(0, H.shape[0] - 1)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "    fig.suptitle(f'plot for row-index: {ROW_INDEX_TO_PLOT}')\n",
    "\n",
    "    # plot histogram\n",
    "    f_x1 = H[ROW_INDEX_TO_PLOT,:]\n",
    "    x1 = np.array(range(len(f_x1)))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Histogram (alpha={:.2f})'.format(y_train[0]))\n",
    "    plt.xlabel('x1 = sample value')\n",
    "    plt.ylabel('f(x1) = number of samples')\n",
    "    _ = plt.plot(x1, f_x1, zorder=2)\n",
    "    _ = plt.scatter(x1, f_x1, zorder=1, s=2, color=\"blue\")\n",
    "\n",
    "    # plot log scale\n",
    "    # (shift H values by one so as not to take log of zero)\n",
    "    log_H = np.apply_along_axis(lambda a: np.log10(a), 1, H + 1)\n",
    "\n",
    "    x2 = np.log10(x1+1)\n",
    "    f_x2 = log_H[ROW_INDEX_TO_PLOT,:]\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('Log scale')\n",
    "    plt.xlabel('x2 = log ( sample value )')\n",
    "    plt.ylabel('f(x2) = log ( number of samples )')\n",
    "    _ = plt.plot(x2, f_x2, zorder=2)\n",
    "    _ = plt.scatter(x2, f_x2, zorder=1, s=2, color=\"blue\")\n",
    "\n",
    "    # plot cumsum\n",
    "    x3 = x2\n",
    "    f_x3 = np.cumsum(f_x2[::-1])[::-1]\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title('Cumsum')\n",
    "    plt.xlabel('x3 = x2 = log ( sample value )')\n",
    "    plt.ylabel('f(x3) = cumsum ( f_x2 )')\n",
    "    _ = plt.plot(x3, f_x3, zorder=2)\n",
    "    _ = plt.scatter(x3, f_x3, zorder=1, s=2, color=\"blue\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
