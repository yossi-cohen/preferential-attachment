{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is a library\n",
    "### It contains the followin functions:\n",
    "1. **generate_data**: generates data for a learning process by means of simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, MaxPooling1D, Dropout, BatchNormalization, Conv1D, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, M, nextConfig, sample):\n",
    "    \"\"\"\n",
    "    This function generates data for a learning process by means of simulation.\n",
    "    \n",
    "    The data consists of ITEMS. Each ITEM is a pair: histogram and a configuration \n",
    "    (for now, the configuration is scalar / a floating point value).\n",
    "    \n",
    "    A histogram consists of sample points. \n",
    "    Each sample points is drawn from a distribution, with some specific configuration.\n",
    "    \n",
    "    Parameter setting for distribution = Configuration, eg.., alpha, gamma, alpha+LOC\n",
    "    \n",
    "    Returns:\n",
    "        - a matrix of histograms and \n",
    "          an array of corresponding configs (one config for each histogram row).\n",
    "\n",
    "    Arguments:\n",
    "        - N: number of ITEMS to generate (rows)\n",
    "        - M: number of sample points that make a histogram (columns)\n",
    "        - nextConfig: A generator that gives a new alpha/gamma/delta, in each call.\n",
    "            Example: nextConfig()\n",
    "                lambda = random(1,10)\n",
    "                return lambda\n",
    "        - sample(config, size): returns sample data points from a distribution (size = num samples).\n",
    "            Example sample(alpha, size=256) \n",
    "                ys = yulesimon(alpha, size)\n",
    "                return ys\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    items_matrix = np.zeros((N, M), dtype=int)\n",
    "    config_array = np.zeros((N,), dtype=float)\n",
    "    \n",
    "    # generate items_matrix:\n",
    "    #   repeat N times:\n",
    "    #     draw M sample points from distribution\n",
    "    for i in range(N):\n",
    "        \n",
    "        # start item\n",
    "        if callable(nextConfig):\n",
    "            config = nextConfig()\n",
    "        else:\n",
    "            config = nextConfig # in case we pass a numeric config\n",
    "\n",
    "        # sample data points with current config and add to sample_matrix\n",
    "        items_matrix[i, :] = sample(config, size=M)\n",
    "        \n",
    "        # append corresponding config\n",
    "        config_array[i] = config\n",
    "    \n",
    "    # create histograms from rows of items_matrix\n",
    "    nbins = np.max(items_matrix)\n",
    "        \n",
    "    histogram_matrix = np.apply_along_axis(\n",
    "        lambda a: np.histogram(a, bins=nbins, range=(1,nbins))[0], 1, items_matrix)\n",
    "\n",
    "    return items_matrix, histogram_matrix, config_array\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log scale & normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_scale(H, C=0):\n",
    "    # log scale H rows\n",
    "    # (shift H values by one so as not to take log of zero)\n",
    "    return np.apply_along_axis(lambda a: np.log10(a + 1 + C), 1, H)\n",
    "\n",
    "def normalize(H):\n",
    "    # normalize values (sum to 1)\n",
    "    return H / H.sum(axis=1, keepdims=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn_model(n_features, \n",
    "                 layers, \n",
    "                 activation='relu', \n",
    "                 init='he_uniform', \n",
    "                 batch_normalization=False, \n",
    "                 dropout=0, \n",
    "                 optimizer='adam', \n",
    "                 k_reg=False, \n",
    "                 k_reg_lr=0.001, \n",
    "                 a_reg=False, \n",
    "                 a_reg_lr=0.001):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ============\n",
    "    # input-layer\n",
    "    # ============\n",
    "    model.add(Dense(units=layers[0]\n",
    "                      , input_dim=n_features\n",
    "                      , kernel_initializer=init\n",
    "                      , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                      , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                      , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # ==============\n",
    "    # hidden-layers\n",
    "    # ==============\n",
    "    for units in layers[1:]:\n",
    "        model.add(Dense(units=units\n",
    "                        , kernel_initializer=init\n",
    "                        , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                        , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                        , use_bias= (not batch_normalization)\n",
    "                        ))\n",
    "\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # =============\n",
    "    # output-layer\n",
    "    # =============\n",
    "    model.add(Dense(units=1\n",
    "                    , kernel_initializer=init\n",
    "                    , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                    , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                    , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mse', metrics=['mse'], optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(n_features, filters=32):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(n_features, 1)))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, batch_size=32):\n",
    "\n",
    "    # split train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\n",
    "\n",
    "    # early-stopping\n",
    "    es_patience = 50\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                        patience=es_patience, \n",
    "                        mode='min', \n",
    "                        restore_best_weights=True, \n",
    "                        verbose=0)\n",
    "    \n",
    "    # reduce learning-rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10)\n",
    "    \n",
    "    # model checkpoint\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    date_str = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')   \n",
    "    base_path           = f'models/DNN_{date_str}'\n",
    "    model_path          = f'{base_path}.h5'\n",
    "    history_path        = f'{base_path}.history'\n",
    "    cp = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode='min', save_best_only=True, verbose=0)\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        epochs=200, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        callbacks=[es, reduce_lr, cp], \n",
    "                        verbose=0)\n",
    "    \n",
    "    # save history with model\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # load best weights from last checkpoint\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, train_key='loss', val_key='val_loss'):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.plot(history[train_key])\n",
    "    plt.plot(history[val_key])\n",
    "    plt.title('learning curves')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_trial(X_train, y_train, X_test, y_test):\n",
    "    dnn_model = create_dnn_model(X_train.shape[1], \n",
    "                            layers=[256,256], \n",
    "                            activation='relu', \n",
    "                            init=keras.initializers.HeUniform(), \n",
    "                            batch_normalization=True, \n",
    "                            dropout=0.0, \n",
    "                            optimizer=Adam(learning_rate=1e-2), \n",
    "                            k_reg=True,\n",
    "                            k_reg_lr=1e-5, \n",
    "                            a_reg=True,\n",
    "                            a_reg_lr=1e-5)\n",
    "\n",
    "    # train\n",
    "    dnn_model, history = train_model(dnn_model, X_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_pred = dnn_model.predict(X_test).flatten()\n",
    "\n",
    "    # mse\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # sqrt_mse\n",
    "    sqrt_mse = np.sqrt(mse)\n",
    "\n",
    "    return dnn_model, history, y_pred, sqrt_mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
