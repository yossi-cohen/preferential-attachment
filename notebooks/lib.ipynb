{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6e118a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aef392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Math, Latex\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras;\n",
    "from tensorflow.keras.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, MaxPooling1D, Dropout, BatchNormalization, Conv1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fix random_state ?\n",
    "RANDOM_STATE = None\n",
    "RS = None\n",
    "\n",
    "def reset_random_state(random_state=None):\n",
    "    if None == random_state:\n",
    "        RANDOM_STATE = None\n",
    "        RS = None\n",
    "        return\n",
    "    \n",
    "    RANDOM_STATE = random_state\n",
    "    RS = np.random.RandomState(RANDOM_STATE)\n",
    "    os.environ['PYTHONHASHSEED']=str(RANDOM_STATE)\n",
    "    tf.random.set_seed(RANDOM_STATE)\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    random.seed(RANDOM_STATE)\n",
    "\n",
    "reset_random_state(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64243cf",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdc510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_scale(H, C=0):\n",
    "    # log scale H rows, shift H values by (1.0+C) to avoid taking log(0)\n",
    "    axis = len(H.shape) - 1 # 1 if 2D 0 if 1D\n",
    "    return np.apply_along_axis(func1d=lambda a: np.log10(a + 1.0 + C), axis=axis, arr=H)\n",
    "\n",
    "def generate_data(N, M, nextConfig, sample, \n",
    "                  density=False, \n",
    "                  dense_histogram=True, \n",
    "                  apply_log_scale=False):\n",
    "    \"\"\"\n",
    "    This function generates data for a learning process by means of simulation.\n",
    "    \n",
    "    The data consists of ITEMS. Each ITEM is a pair: histogram and a configuration \n",
    "    (for now, the configuration is scalar / a floating point value).\n",
    "    \n",
    "    A histogram consists of sample points. \n",
    "    Each sample points is drawn from a distribution, with some specific configuration.\n",
    "    \n",
    "    Parameter setting for distribution = Configuration, eg.., alpha, gamma, alpha+LOC\n",
    "    \n",
    "    Returns:\n",
    "        - a matrix of histograms and \n",
    "          an array of corresponding configs (one config for each histogram row).\n",
    "\n",
    "    Arguments:\n",
    "        - N: number of ITEMS to generate (rows)\n",
    "        - M: number of sample points that make a histogram (columns)\n",
    "        - nextConfig: A generator that gives a new alpha/gamma/delta, in each call.\n",
    "            Example: nextConfig()\n",
    "                lambda = random(1,10)\n",
    "                return lambda\n",
    "        - sample(config, size): returns sample data points from a distribution (size = num samples).\n",
    "            Example sample(alpha, size=256) \n",
    "                ys = yulesimon(alpha, size)\n",
    "                return ys\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate samples:\n",
    "    samples = np.zeros((N, M), dtype=int)\n",
    "    config_array = np.zeros((N,), dtype=float)\n",
    "    \n",
    "    #   repeat N times:\n",
    "    #     draw M sample points from distribution\n",
    "    for i in range(N):\n",
    "        \n",
    "        # start item\n",
    "        if callable(nextConfig):\n",
    "            config = nextConfig()\n",
    "        else:\n",
    "            config = nextConfig # in case we pass a numeric config\n",
    "\n",
    "        # sample data points with current config and add to sample_matrix\n",
    "        samples[i, :] = sample(config, size=M)\n",
    "        \n",
    "        # append corresponding config\n",
    "        config_array[i] = config\n",
    "    \n",
    "    # create a histogram for each row\n",
    "    if dense_histogram:\n",
    "        nbins = np.max(samples)\n",
    "    else:\n",
    "        nbins = M\n",
    "    \n",
    "    histogram = np.apply_along_axis(\n",
    "       lambda a: np.histogram(a, bins=nbins, range=(0,nbins), density=density)[0], 1, samples)\n",
    "\n",
    "    if apply_log_scale:\n",
    "        histogram = log_scale(histogram)\n",
    "    \n",
    "    return samples, histogram, config_array\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564cf0f",
   "metadata": {},
   "source": [
    "## Generate data (poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae169fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import poisson\n",
    "\n",
    "def sample_poisson(mu, size):\n",
    "    return poisson.rvs(mu=mu, loc=0, size=size, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def next_lambda(min_lambda=0.1, max_lambda=10):\n",
    "    if None != RS:\n",
    "        return RS.uniform(low=min_lambda, high=max_lambda, size=1)[0]\n",
    "    return np.random.uniform(low=min_lambda, high=max_lambda, size=1)[0]\n",
    "\n",
    "def generate_data_poisson(N, M, density=True):\n",
    "\n",
    "    # generate histogram and corresponding lambda values\n",
    "    raw, H, lambdas = generate_data(N=N, M=M, nextConfig=next_lambda, sample=sample_poisson, density=density)\n",
    "\n",
    "    # split train/test\n",
    "    # (use train_test_split so the shape of the train/test data will be the same)\n",
    "    H_train, H_test, y_train, y_test = train_test_split(H, lambdas, test_size=0.25, random_state=RANDOM_STATE)\n",
    "    \n",
    "    return H_train, y_train, H_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902af362",
   "metadata": {},
   "source": [
    "# DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf82484",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ACTIVATION = 'relu'\n",
    "\n",
    "def create_dnn_model(n_features, \n",
    "                 layers, \n",
    "                 activation=DEFAULT_ACTIVATION, \n",
    "                 init='he_uniform', \n",
    "                 batch_normalization=False, \n",
    "                 dropout=0, \n",
    "                 optimizer='adam', \n",
    "                 k_reg=False, \n",
    "                 k_reg_lr=0.001, \n",
    "                 a_reg=False, \n",
    "                 a_reg_lr=0.001):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ============\n",
    "    # input-layer\n",
    "    # ============\n",
    "    model.add(Dense(units=layers[0]\n",
    "                      , input_dim=n_features\n",
    "                      , kernel_initializer=init\n",
    "                      , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                      , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                      , use_bias=(not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    if dropout >= 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # ==============\n",
    "    # hidden-layers\n",
    "    # ==============\n",
    "    for units in layers[1:]:\n",
    "        model.add(Dense(units=units\n",
    "                        , kernel_initializer=init\n",
    "                        , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                        , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                        , use_bias=(not batch_normalization)\n",
    "                        ))\n",
    "\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # =============\n",
    "    # output-layer\n",
    "    # =============\n",
    "    model.add(Dense(units=1\n",
    "                    , kernel_initializer=init\n",
    "                    , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                    , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                    , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mse', metrics=['mse'], optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5bef60",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7185e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(n_features, filters=32):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(32, 3, activation=DEFAULT_ACTIVATION, input_shape=(n_features, 1)))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(64, activation=DEFAULT_ACTIVATION))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa939fe7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d187a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, batch_size=32):\n",
    "\n",
    "    # split train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                      y_train, \n",
    "                                                      test_size=0.25, \n",
    "                                                      random_state=RANDOM_STATE)\n",
    "\n",
    "    # early-stopping\n",
    "    es_patience = 50\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                        patience=es_patience, \n",
    "                        mode='min', \n",
    "                        restore_best_weights=True, \n",
    "                        verbose=0)\n",
    "    \n",
    "    # reduce learning-rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10)\n",
    "    \n",
    "    # model checkpoint\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    date_str = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')   \n",
    "    base_path           = f'models/DNN_{date_str}'\n",
    "    model_path          = f'{base_path}.h5'\n",
    "    history_path        = f'{base_path}.history'\n",
    "    \n",
    "    cp = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode='min', save_best_only=True, verbose=0)\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        epochs=200, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        callbacks=[es, reduce_lr, cp], \n",
    "                        verbose=0)\n",
    "    \n",
    "    # save history with model\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # load best weights from last checkpoint\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ab375",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f558d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, train_key='loss', val_key='val_loss'):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.plot(history[train_key])\n",
    "    plt.plot(history[val_key])\n",
    "    plt.title('learning curves')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6c8fa",
   "metadata": {},
   "source": [
    "# Create and train DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cba58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_trial(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    dnn_model = create_dnn_model(X_train.shape[1], \n",
    "                            layers=[256,256], \n",
    "                            activation=DEFAULT_ACTIVATION, \n",
    "                            init=keras.initializers.HeUniform(), \n",
    "                            batch_normalization=True, \n",
    "                            dropout=0.0, \n",
    "                            optimizer=Adam(learning_rate=1e-2), \n",
    "                            k_reg=True,\n",
    "                            k_reg_lr=1e-5, \n",
    "                            a_reg=True,\n",
    "                            a_reg_lr=1e-5)\n",
    "    # train\n",
    "    dnn_model, history = train_model(dnn_model, X_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_pred = dnn_model.predict(X_test).flatten()\n",
    "\n",
    "    # mse\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # sqrt_mse\n",
    "    sqrt_mse = np.sqrt(mse)\n",
    "\n",
    "    return dnn_model, history, y_pred, sqrt_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c5a2e",
   "metadata": {},
   "source": [
    "# Confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ce9548",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
