{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b4c526",
   "metadata": {},
   "source": [
    "# This notebook is a library\n",
    "### It contains the followin functions:\n",
    "1. **generate_data**: generates data for a learning process by means of simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792771f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6fb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, MaxPooling1D, Dropout, BatchNormalization, Conv1D, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd3df7",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005a91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, M, nextConfig, sample):\n",
    "    \"\"\"\n",
    "    This function generates data for a learning process by means of simulation.\n",
    "    \n",
    "    The data consists of ITEMS. Each ITEM is a pair: histogram and a configuration \n",
    "    (for now, the configuration is scalar / a floating point value).\n",
    "    \n",
    "    A histogram consists of sample points. \n",
    "    Each sample points is drawn from a distribution, with some specific configuration.\n",
    "    \n",
    "    Parameter setting for distribution = Configuration, eg.., alpha, gamma, alpha+LOC\n",
    "    \n",
    "    Returns:\n",
    "        - a matrix of histograms and \n",
    "          an array of corresponding configs (one config for each histogram row).\n",
    "\n",
    "    Arguments:\n",
    "        - N: number of ITEMS to generate (rows)\n",
    "        - M: number of sample points that make a histogram (columns)\n",
    "        - nextConfig: A generator that gives a new alpha/gamma/delta, in each call.\n",
    "            Example: nextConfig()\n",
    "                lambda = random(1,10)\n",
    "                return lambda\n",
    "        - sample(config, size): returns sample data points from a distribution (size = num samples).\n",
    "            Example sample(alpha, size=256) \n",
    "                ys = yulesimon(alpha, size)\n",
    "                return ys\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    items_matrix = np.zeros((N, M), dtype=int)\n",
    "    config_array = np.zeros((N,), dtype=float)\n",
    "    \n",
    "    # generate items_matrix:\n",
    "    #   repeat N times:\n",
    "    #     draw M sample points from distribution\n",
    "    for i in range(N):\n",
    "        \n",
    "        # start item\n",
    "        config = nextConfig()\n",
    "\n",
    "        # sample data points with current config and add to sample_matrix\n",
    "        items_matrix[i, :] = sample(config, size=M)\n",
    "        \n",
    "        # append corresponding config\n",
    "        config_array[i] = config\n",
    "    \n",
    "    # create histograms from rows of items_matrix\n",
    "    nbins = np.max(items_matrix)\n",
    "#     histogram_matrix = np.apply_along_axis(\n",
    "#         lambda a: np.histogram(a, bins=nbins)[0], 1, items_matrix)\n",
    "#     histogram_matrix = np.apply_along_axis(\n",
    "#         lambda a: np.histogram(a, bins=nbins, range=(1,np.max(a)))[0], 1, items_matrix)\n",
    "    histogram_matrix = np.apply_along_axis(\n",
    "        lambda a: np.histogram(a, bins=nbins, range=(1,nbins))[0], 1, items_matrix)\n",
    "\n",
    "    return histogram_matrix, config_array\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b261a6c",
   "metadata": {},
   "source": [
    "# Create DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1416cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn_model(n_features, \n",
    "                 layers, \n",
    "                 activation='relu', \n",
    "                 init='he_uniform', \n",
    "                 batch_normalization=False, \n",
    "                 dropout=0, \n",
    "                 optimizer='adam', \n",
    "                 k_reg=False, \n",
    "                 k_reg_lr=0.001, \n",
    "                 a_reg=False, \n",
    "                 a_reg_lr=0.001):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # ============\n",
    "    # input-layer\n",
    "    # ============\n",
    "    model.add(Dense(units=layers[0]\n",
    "                      , input_dim=n_features\n",
    "                      , kernel_initializer=init\n",
    "                      , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                      , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                      , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # ==============\n",
    "    # hidden-layers\n",
    "    # ==============\n",
    "    for units in layers[1:]:\n",
    "        model.add(Dense(units=units\n",
    "                        , kernel_initializer=init\n",
    "                        , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                        , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                        , use_bias= (not batch_normalization)\n",
    "                        ))\n",
    "\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    # =============\n",
    "    # output-layer\n",
    "    # =============\n",
    "    model.add(Dense(units=1\n",
    "                    , kernel_initializer=init\n",
    "                    , kernel_regularizer=l2(k_reg_lr) if k_reg else None\n",
    "                    , activity_regularizer=l2(a_reg_lr) if a_reg else None\n",
    "                    , use_bias= (not batch_normalization)\n",
    "                    ))\n",
    "    \n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mse', metrics=['mse'], optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e262998",
   "metadata": {},
   "source": [
    "# Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dnn_model(model, X_train, y_train, batch_size=32):\n",
    "\n",
    "    # split train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\n",
    "\n",
    "    # early-stopping\n",
    "    es_patience = 50\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                        patience=es_patience, \n",
    "                        mode='min', \n",
    "                        restore_best_weights=True, \n",
    "                        verbose=0)\n",
    "    \n",
    "    # reduce learning-rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        epochs=200, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        callbacks=[es, reduce_lr], \n",
    "                        verbose=0)\n",
    "    \n",
    "    # save history with model\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # load best weights from last checkpoint\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model, history.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
