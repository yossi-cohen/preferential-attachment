{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import yulesimon\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, MaxPooling1D, Dropout, BatchNormalization, Conv1D, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "# fix random seed for reproducability\n",
    "def fix_random(seed):\n",
    "    os.environ['PYTONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "source": [
    "# Generate data\n",
    "### &nbsp;&nbsp; *num_alphas*: number of alphas generated between (min_alpha) and (max_alpha) inclusive\n",
    "### &nbsp;&nbsp; *samples_per_alpha*: number of samples (rows) for each alpha \n",
    "### &nbsp;&nbsp; *N* : number of random variates (number of samples drawn from yulesimon distribution)\n",
    "### &nbsp;&nbsp; *M* : maximun value of random variates (length of input vectors == number of features)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_M(N, \n",
    "                  num_alphas=100, \n",
    "                  samples_per_alpha=100, \n",
    "                  min_alpha=2.01, \n",
    "                  max_alpha=3.00, \n",
    "                  loc=0, \n",
    "                  random_alpha=False, \n",
    "                  random_state=0):\n",
    "    max_M = 0\n",
    "    \n",
    "    if random_alpha:\n",
    "        alphas = np.random.uniform(low=min_alpha, high=max_alpha, size=num_alphas)\n",
    "    else:\n",
    "        alphas = np.linspace(min_alpha, max_alpha, num=num_alphas)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        for i in range(samples_per_alpha):\n",
    "            X = yulesimon.rvs(alpha, loc=loc, size=N, random_state=random_state)\n",
    "            max_M = max(max_M, np.max(X))\n",
    "\n",
    "    return max_M\n",
    "\n",
    "def generate_data(N, \n",
    "                  num_alphas=100, \n",
    "                  samples_per_alpha=100, \n",
    "                  min_alpha=2.01, \n",
    "                  max_alpha=3.00, \n",
    "                  loc=0, \n",
    "                  random_alpha=False, \n",
    "                  random_state=0):\n",
    "    '''\n",
    "    params:\n",
    "        num_alphas: number of alphas generated between (min_alpha) and (max_alpha) inclusive\n",
    "        samples_per_alpha: number of samples (rows) for each alpha \n",
    "        N: number of RV samples (columns) per row\n",
    "    '''\n",
    "\n",
    "    X = np.empty((num_alphas * samples_per_alpha, N+1), float)\n",
    "\n",
    "    row = 0\n",
    "    \n",
    "    if random_alpha:\n",
    "        alphas = np.random.uniform(low=min_alpha, high=max_alpha, size=num_alphas)\n",
    "    else:\n",
    "        alphas = np.linspace(min_alpha, max_alpha, num=num_alphas)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        \n",
    "        # generate samples (rows) for current alpha\n",
    "        for i in range(samples_per_alpha):\n",
    "            X[row, 0] = alpha\n",
    "            X[row, 1:] = yulesimon.rvs(alpha, loc=loc, size=N, random_state=random_state)\n",
    "            row += 1\n",
    "\n",
    "    # suffle rows\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    # separate X from y\n",
    "    y = X[:, 0]\n",
    "    X = X[:, 1:].astype(int)\n",
    "\n",
    "    # create a histogram (H) from (X) rows\n",
    "    nbins = np.max(X)\n",
    "    H = np.apply_along_axis(lambda a: np.histogram(a, bins=nbins, density=False)[0], 1, X)\n",
    "\n",
    "    # log scale (H) rows\n",
    "    logH = np.apply_along_axis(lambda a: np.log10(a+1), 1, H)\n",
    "\n",
    "    return logH, y, nbins # (nbins == M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(n_features, filters=32):\n",
    "    \n",
    "    model = Sequential()\n",
    "    # model.add(Conv1D(32, 2, activation=\"relu\", input_shape=(n_features,1)))\n",
    "    model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(n_features, 1)))\n",
    "    \n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, filters=32, batch_size=32, random_state=0):\n",
    "\n",
    "    # split train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "        test_size=0.25, random_state=random_state)\n",
    "\n",
    "    # create model\n",
    "    model = create_cnn_model(X_train.shape[1], filters=filters)\n",
    "    \n",
    "    # early-stopping\n",
    "    es_patience = 50\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                        patience=es_patience, \n",
    "                        mode='min', \n",
    "                        restore_best_weights=True, \n",
    "                        verbose=0)\n",
    "    \n",
    "    # model checkpoint\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    date_str = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "    base_path       = 'models/yulesimon_{}'.format(date_str)\n",
    "    model_path      = '{}.h5'.format(base_path)\n",
    "    history_path    = '{}.history'.format(base_path)\n",
    "    \n",
    "    cp = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode='min', save_best_only=True, verbose=0)\n",
    "    \n",
    "    # reduce learning-rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        epochs=200, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        callbacks=[es, reduce_lr, cp], \n",
    "                        verbose=0)\n",
    "    \n",
    "    # save history with model\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # load best weights from last checkpoint\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, train_key='loss', val_key='val_loss'):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.plot(history[train_key])\n",
    "    plt.plot(history[val_key])\n",
    "    plt.title('learning curves')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()\n",
    "    print('loss:', np.min(history['loss']))\n",
    "    print('val_loss:', np.min(history['val_loss']))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}