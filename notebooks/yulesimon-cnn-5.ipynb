{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import yulesimon\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, Conv1D, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "# fix random seed for reproducability\n",
    "def fix_random(seed):\n",
    "    os.environ['PYTONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "source": [
    "# Generate data\n",
    "### &nbsp;&nbsp; *num_alphas*: number of alphas generated between (min_alpha) and (max_alpha) inclusive\n",
    "### &nbsp;&nbsp; *samples_per_alpha*: number of samples (rows) for each alpha \n",
    "### &nbsp;&nbsp; *N* : number of random variates (number of samples drawn from yulesimon distribution)\n",
    "### &nbsp;&nbsp; *M* : maximun value of random variates (length of input vectors == number of features)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_data(num_alphas, samples_per_alpha, N, min_alpha=2.01, max_alpha=3.00, random_alpha=False, random_state=0):\n",
    "    '''\n",
    "    params:\n",
    "        num_alphas: number of alphas generated between (min_alpha) and (max_alpha) inclusive\n",
    "        samples_per_alpha: number of samples (rows) for each alpha \n",
    "        N: number of RV samples (columns) per row\n",
    "    '''\n",
    "\n",
    "    # fix loc at zero\n",
    "    loc = 0\n",
    "\n",
    "    X = np.empty((num_alphas * samples_per_alpha, N+1), float)\n",
    "\n",
    "    row = 0\n",
    "    \n",
    "    if random_alpha:\n",
    "        alphas = np.random.uniform(low=min_alpha, high=max_alpha, size=num_alphas)\n",
    "    else:\n",
    "        alphas = np.linspace(min_alpha, max_alpha, num=num_alphas)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        \n",
    "        # generate samples (rows) for current alpha\n",
    "        for i in range(samples_per_alpha):\n",
    "            X[row, 0] = alpha\n",
    "            X[row, 1:] = yulesimon.rvs(alpha, loc=loc, size=N, random_state=random_state)\n",
    "            row += 1\n",
    "\n",
    "    # suffle rows\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    # separate X from y\n",
    "    y = X[:, 0]\n",
    "    X = X[:, 1:].astype(int)\n",
    "\n",
    "    # create a histogram (H) from (X) rows\n",
    "    nbins = np.max(X)\n",
    "    H = np.apply_along_axis(lambda a: np.histogram(a, bins=nbins, density=False)[0], 1, X)\n",
    "\n",
    "    # log scale (H) rows\n",
    "    logH = np.apply_along_axis(lambda a: np.log10(a+1), 1, H)\n",
    "\n",
    "    return logH, y, nbins # (nbins == M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_features, filters=32):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters, 2, activation=\"relu\", input_shape=(n_features,1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, filters=32, batch_size=32, random_state=0):\n",
    "\n",
    "    # split train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "        test_size=0.25, random_state=random_state)\n",
    "\n",
    "    # create model\n",
    "    model = create_model(X_train.shape[1], filters=filters)\n",
    "    \n",
    "    # early-stopping\n",
    "    es_patience = 50\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                        patience=es_patience, \n",
    "                        mode='min', \n",
    "                        restore_best_weights=True, \n",
    "                        verbose=0)\n",
    "    \n",
    "    # model checkpoint\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    date_str = datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "    base_path       = 'models/yulesimon_{}'.format(date_str)\n",
    "    model_path      = '{}.h5'.format(base_path)\n",
    "    history_path    = '{}.history'.format(base_path)\n",
    "    \n",
    "    cp = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode='min', save_best_only=True, verbose=0)\n",
    "    \n",
    "    # reduce learning-rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        epochs=200, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        callbacks=[es, reduce_lr, cp], \n",
    "                        verbose=0)\n",
    "    \n",
    "    # save history with model\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # load best weights from last checkpoint\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, train_key='loss', val_key='val_loss'):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.plot(history[train_key])\n",
    "    plt.plot(history[val_key])\n",
    "    plt.title('learning curves')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()\n",
    "    print('loss:', np.min(history['loss']))\n",
    "    print('val_loss:', np.min(history['val_loss']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trial(random_alpha=False):\n",
    "\n",
    "    print('Processing...')\n",
    "    print()\n",
    "\n",
    "    a_N = np.array([])\n",
    "    a_sqrt_mse = np.array([])\n",
    "\n",
    "    # 10 random states\n",
    "    random_states = [0, 3, 5, 8, 11, 16, 17, 20, 21, 24]\n",
    "\n",
    "    # hold array of absolute errors\n",
    "    abs_errors = np.array([], dtype=float)\n",
    "\n",
    "    # change N [32..2048]\n",
    "    for i in range(5, 12):\n",
    "\n",
    "        N = 2**i\n",
    "\n",
    "        a_N = np.append(a_N, N)\n",
    "\n",
    "        X, y, M = generate_data(num_alphas=100, samples_per_alpha=100, N=N, random_alpha=random_alpha, random_state=0)\n",
    "\n",
    "        # reshape X for Conv1D\n",
    "        X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "        # split train/test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "        avg_sqrt_mse = 0\n",
    "\n",
    "        # change random state\n",
    "        for rs in random_states:\n",
    "\n",
    "            # fix random\n",
    "            fix_random(seed=rs)\n",
    "        \n",
    "            # training\n",
    "            model, history = train(X_train, y_train, filters=32, batch_size=32, random_state=rs)\n",
    "\n",
    "            # predict\n",
    "            y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "            # mse\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "            # sqrt_mse\n",
    "            sqrt_mse = np.sqrt(mse)\n",
    "\n",
    "            # avg_sqrt_mse (accumulator)\n",
    "            avg_sqrt_mse += sqrt_mse\n",
    "            \n",
    "            print('N = {}, M = {}, random_state = {}    =>    sqrt_mse = {:.6f}'.format(N, M, rs, sqrt_mse))\n",
    "\n",
    "            # absolute errors\n",
    "            abs_errors = np.append(abs_errors, np.abs(y_test - y_pred))\n",
    "\n",
    "        # avg_sqrt_mse\n",
    "        avg_sqrt_mse = avg_sqrt_mse / len(random_states)\n",
    "        a_sqrt_mse = np.append(a_sqrt_mse, avg_sqrt_mse)\n",
    "        print('N = {}, M = {}    =>    avg_sqrt_mse = {:.6f}'.format(N, M, avg_sqrt_mse))\n",
    "        print()\n",
    "\n",
    "    return abs_errors, a_N, a_sqrt_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abs_errors_1, a_N_1, a_sqrt_mse_1 = trial(random_alpha=False)\n",
    "abs_errors_2, a_N_2, a_sqrt_mse_2 = trial(random_alpha=True)"
   ]
  },
  {
   "source": [
    "# plot log(N) vs sqrt_mse"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sqrt_mse():\n",
    "    \n",
    "    plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax1.set(title='random_alpha = False', xlabel='log(N)', ylabel='sqrt_mse')\n",
    "    ax1.scatter(np.log10(a_N_1), a_sqrt_mse_1)\n",
    "\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2.set(title='random_alpha = True', xlabel='log(N)', ylabel='sqrt_mse')\n",
    "    ax2.scatter(np.log10(a_N_2), a_sqrt_mse_2)\n",
    "\n",
    "plot_sqrt_mse()"
   ]
  },
  {
   "source": [
    "# plot error distribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.distplot(abs_errors_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.distplot(abs_errors_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ou",
   "language": "python",
   "name": "ou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}